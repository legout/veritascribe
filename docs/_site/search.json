[
  {
    "objectID": "usage.html",
    "href": "usage.html",
    "title": "Usage Guide",
    "section": "",
    "text": "This comprehensive guide covers all aspects of using VeritaScribe to analyze thesis documents.\n\n\nVeritaScribe provides several commands for different use cases:\n\n\n\nCommand\nPurpose\nUse Case\n\n\n\n\ndemo\nCreate and analyze sample document\nTesting setup\n\n\nquick\nFast analysis of document subset\nQuick feedback\n\n\nanalyze\nFull document analysis\nComplete review\n\n\nconfig\nView configuration\nTroubleshooting\n\n\ntest\nSystem diagnostics\nVerify installation\n\n\n\n\n\n\n\n\nStart with the demo to familiarize yourself with VeritaScribe:\nuv run python -m veritascribe demo\nThis command will: - Create a sample thesis PDF (demo_thesis.pdf) - Perform quick analysis if API key is configured - Show example output and reports\n\n\n\nFor rapid feedback on your document:\nuv run python -m veritascribe quick your_thesis.pdf\nQuick analysis: - Analyzes first 5 text blocks by default - Provides immediate feedback - Useful during writing process - Lower API costs\nCustomize block count:\nuv run python -m veritascribe quick your_thesis.pdf --blocks 10\n\n\n\nFor comprehensive document review:\nuv run python -m veritascribe analyze your_thesis.pdf\nThis performs: - Complete document analysis - All error types detection - Detailed reporting - Visualization generation\n\n\n\n\n\n\nThe primary command for comprehensive thesis analysis.\n\n\nuv run python -m veritascribe analyze thesis.pdf\n\n\n\nuv run python -m veritascribe analyze thesis.pdf \\\n  --output ./results \\\n  --citation-style APA \\\n  --verbose\n\n\n\n\n\n\nOption\nShort\nDescription\nDefault\n\n\n\n\n--output\n-o\nOutput directory\n./analysis_output\n\n\n--citation-style\n-c\nCitation style\nAPA\n\n\n--quick\n-q\nQuick mode (10 blocks)\nfalse\n\n\n--no-viz\n\nSkip visualizations\nfalse\n\n\n--verbose\n-v\nVerbose logging\nfalse\n\n\n\n\n\n\n# American Psychological Association\n--citation-style APA\n\n# Modern Language Association  \n--citation-style MLA\n\n# Chicago Manual of Style\n--citation-style Chicago\n\n# IEEE format\n--citation-style IEEE\n\n# Harvard referencing\n--citation-style Harvard\n\n\n\nStandard Analysis:\nuv run python -m veritascribe analyze thesis.pdf\nCustom Output Location:\nuv run python -m veritascribe analyze thesis.pdf \\\n  --output ~/Documents/thesis_review\nMLA Citation Style:\nuv run python -m veritascribe analyze thesis.pdf \\\n  --citation-style MLA\nQuick Full Analysis:\nuv run python -m veritascribe analyze thesis.pdf --quick\nAnalysis Without Visualizations:\nuv run python -m veritascribe analyze thesis.pdf --no-viz\n\n\n\n\nIdeal for iterative writing and quick feedback.\n\n\nuv run python -m veritascribe quick thesis.pdf\n\n\n\n# Analyze first 3 blocks\nuv run python -m veritascribe quick thesis.pdf --blocks 3\n\n# Analyze first 15 blocks  \nuv run python -m veritascribe quick thesis.pdf --blocks 15\n\n\n\n\nDuring writing: Get feedback on recent sections\nInitial review: Spot-check document quality\nCost management: Reduce API usage for large documents\nDebugging: Test configuration with small sample\n\n\n\n\n\n\n\n\nVeritaScribe provides rich console output with progress indicators and summaries:\nStarting analysis of: thesis.pdf\nOutput directory: ./analysis_output\n\nAnalyzing document... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100%\n\nAnalysis Results: thesis.pdf\n┌─────────────────────────────────────────────────────────────────────────────────┐\n│ 📄 Pages: 45                                                                    │\n│ 📝 Words: 12,543                                                               │\n│ 🔍 Text blocks analyzed: 87                                                     │\n│ ⚠️  Total errors: 23                                                            │\n│ 📊 Error rate: 1.83 per 1,000 words                                           │\n│ ⏱️  Processing time: 45.32s                                                     │\n└─────────────────────────────────────────────────────────────────────────────────┘\n\nErrors by Type\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Type                                                                             ┃ Count                                                                            ┃ Percentage                                                                       ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ Grammar                                                                          │ 12                                                                               │ 52.2%                                                                           │\n│ Citation Format                                                                  │ 7                                                                                │ 30.4%                                                                           │\n│ Content Plausibility                                                             │ 4                                                                                │ 17.4%                                                                           │\n└──────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────┘\n\nSeverity Breakdown: 🔴 High: 3 | 🟡 Medium: 8 | 🟢 Low: 12\n\n🚨 3 high-priority issues require immediate attention!\n\n✓ Analysis completed successfully!\n\nGenerated files:\n  📄 Text report: ./analysis_output/thesis_20240315_143022_report.md\n  📊 JSON data: ./analysis_output/thesis_20240315_143022_data.json\n  📈 Visualizations: ./analysis_output/thesis_20240315_143022_visualizations/\n\nRecommendation: Document shows good overall quality with some areas for improvement. Focus on addressing high-priority grammar and citation issues first.\n\n\n\nEach analysis produces several output files:\n\n\nComprehensive Markdown report with:\n# VeritaScribe Analysis Report\n\n**Document:** thesis.pdf\n**Analysis Date:** 2024-03-15 14:30:22\n**Processing Time:** 45.32 seconds\n\n## Executive Summary\n\n- **Total Pages:** 45\n- **Word Count:** 12,543\n- **Text Blocks Analyzed:** 87\n- **Total Errors Found:** 23\n- **Error Rate:** 1.83 per 1,000 words\n\n## Error Summary\n\n### By Type\n- Grammar: 12 errors (52.2%)\n- Citation Format: 7 errors (30.4%)  \n- Content Plausibility: 4 errors (17.4%)\n\n### By Severity\n- High Priority: 3 errors\n- Medium Priority: 8 errors\n- Low Priority: 12 errors\n\n## Detailed Findings\n\n### High Priority Issues\n\n#### 1. Grammar Error (Page 12)\n**Location:** Page 12, coordinates (72, 345, 520, 365)\n**Original Text:** \"The results shows that...\"\n**Suggested Fix:** \"The results show that...\"\n**Explanation:** Subject-verb agreement error. 'Results' is plural and requires 'show' not 'shows'.\n\n#### 2. Citation Format Error (Page 23)\n**Location:** Page 23, coordinates (72, 123, 520, 143)\n**Original Text:** \"(Smith 2020)\"\n**Suggested Fix:** \"(Smith, 2020)\"\n**Explanation:** APA format requires comma between author and year.\n\n[... more detailed errors ...]\n\n## Recommendations\n\n1. **Immediate Action Required:** Address 3 high-priority issues\n2. **Grammar Focus:** Review subject-verb agreements throughout\n3. **Citation Consistency:** Ensure all citations follow APA format\n4. **Content Review:** Verify factual claims in identified sections\n\n## Overall Assessment\n\nDocument quality: **Good** with room for improvement.\nRecommended next steps: Focus on high-priority issues first.\n\n\n\nStructured data for programmatic access:\n{\n  \"document_name\": \"thesis.pdf\",\n  \"analysis_timestamp\": \"2024-03-15T14:30:22.123456\",\n  \"total_pages\": 45,\n  \"total_words\": 12543,\n  \"total_text_blocks\": 87,\n  \"total_errors\": 23,\n  \"error_rate\": 1.83,\n  \"total_processing_time_seconds\": 45.32,\n  \"errors_by_type\": {\n    \"grammar\": 12,\n    \"citation_format\": 7,\n    \"content_plausibility\": 4\n  },\n  \"errors_by_severity\": {\n    \"high\": 3,\n    \"medium\": 8,\n    \"low\": 12\n  },\n  \"analysis_results\": [\n    {\n      \"text_block\": {\n        \"content\": \"The results shows that...\",\n        \"page_number\": 12,\n        \"bounding_box\": [72, 345, 520, 365]\n      },\n      \"errors\": [\n        {\n          \"error_type\": \"grammar\",\n          \"severity\": \"high\",\n          \"original_text\": \"The results shows that\",\n          \"suggested_correction\": \"The results show that\",\n          \"explanation\": \"Subject-verb agreement error...\",\n          \"location\": {\n            \"page_number\": 12,\n            \"bounding_box\": [72, 345, 520, 365]\n          }\n        }\n      ]\n    }\n  ]\n}\n\n\n\nCharts and graphs in the visualizations directory:\n\nError Distribution by Type (error_types.png)\nError Density by Page (error_density.png)\nSeverity Breakdown (severity_breakdown.png)\n\n\n\n\n\n\n\n\nProcess multiple documents:\n# Create script for batch processing\ncat &gt; batch_analyze.sh &lt;&lt; 'EOF'\n#!/bin/bash\nfor pdf in *.pdf; do\n  echo \"Analyzing $pdf...\"\n  uv run python -m veritascribe analyze \"$pdf\" \\\n    --output \"./results/$(basename \"$pdf\" .pdf)\"\ndone\nEOF\n\nchmod +x batch_analyze.sh\n./batch_analyze.sh\n\n\n\nDifferent workflows for different academic styles:\n# APA Style (Psychology, Education)\nuv run python -m veritascribe analyze thesis.pdf --citation-style APA\n\n# MLA Style (Literature, Humanities)  \nuv run python -m veritascribe analyze thesis.pdf --citation-style MLA\n\n# Chicago Style (History, Arts)\nuv run python -m veritascribe analyze thesis.pdf --citation-style Chicago\n\n# IEEE Style (Engineering, Computer Science)\nuv run python -m veritascribe analyze thesis.pdf --citation-style IEEE\n\n\n\nWorkflow for document improvement:\n# Step 1: Initial quick review\nuv run python -m veritascribe quick draft.pdf --blocks 10\n\n# Step 2: Address major issues, then full analysis\nuv run python -m veritascribe analyze draft.pdf --output ./review_1\n\n# Step 3: After revisions, re-analyze\nuv run python -m veritascribe analyze revised_draft.pdf --output ./review_2\n\n# Step 4: Compare results\ndiff ./review_1/draft_*_data.json ./review_2/revised_draft_*_data.json\n\n\n\nOptimize API usage for large documents:\n# Strategy 1: Quick analysis first\nuv run python -m veritascribe quick large_thesis.pdf --blocks 20\n\n# Strategy 2: Disable expensive analysis types\nCONTENT_ANALYSIS_ENABLED=false \\\nuv run python -m veritascribe analyze large_thesis.pdf\n\n# Strategy 3: Use cheaper model\nDEFAULT_MODEL=gpt-3.5-turbo \\\nuv run python -m veritascribe analyze large_thesis.pdf\n\n# Strategy 4: Reduce parallel processing\nMAX_CONCURRENT_REQUESTS=2 \\\nuv run python -m veritascribe analyze large_thesis.pdf\n\n\n\n\n\n\nHigh Priority (Score ≥ 0.8): - Critical grammar errors - Major citation format violations - Significant factual inconsistencies - Action: Fix immediately\nMedium Priority (Score ≥ 0.5): - Minor grammar issues - Style inconsistencies - Citation formatting preferences - Action: Review and fix as time permits\nLow Priority (Score &lt; 0.5): - Style suggestions - Minor formatting preferences - Optional improvements - Action: Consider for polish pass\n\n\n\nEach error includes precise location data:\n{\n  \"location\": {\n    \"page_number\": 12,\n    \"bounding_box\": [72, 345, 520, 365]\n  }\n}\nBounding box coordinates [x1, y1, x2, y2]: - (x1, y1): Top-left corner - (x2, y2): Bottom-right corner - Units are in PDF points (72 points = 1 inch)\n\n\n\nProcess results programmatically:\nimport json\n\n# Load analysis results\nwith open('thesis_20240315_143022_data.json', 'r') as f:\n    results = json.load(f)\n\n# Get high-priority errors\nhigh_priority = [\n    error for result in results['analysis_results']\n    for error in result['errors']\n    if error['severity'] == 'high'\n]\n\nprint(f\"Found {len(high_priority)} high-priority issues\")\n\n# Group errors by type\nerror_types = {}\nfor result in results['analysis_results']:\n    for error in result['errors']:\n        error_type = error['error_type']\n        error_types[error_type] = error_types.get(error_type, 0) + 1\n\nprint(\"Error distribution:\", error_types)\n\n\n\n\n\n\nImport location information into editors:\n# Generate editor jump commands\ndef generate_editor_commands(json_file):\n    with open(json_file, 'r') as f:\n        results = json.load(f)\n    \n    commands = []\n    for result in results['analysis_results']:\n        for error in result['errors']:\n            page = error['location']['page_number']\n            text = error['original_text'][:50]\n            commands.append(f\"# Page {page}: {text}...\")\n    \n    return commands\n\ncommands = generate_editor_commands('results.json')\nfor cmd in commands[:5]:  # Show first 5\n    print(cmd)\n\n\n\nExport citation issues for reference managers:\ndef export_citation_issues(json_file):\n    with open(json_file, 'r') as f:\n        results = json.load(f)\n    \n    citation_errors = []\n    for result in results['analysis_results']:\n        for error in result['errors']:\n            if error['error_type'] == 'citation_format':\n                citation_errors.append({\n                    'page': error['location']['page_number'],\n                    'original': error['original_text'],\n                    'suggested': error['suggested_correction'],\n                    'explanation': error['explanation']\n                })\n    \n    return citation_errors\n\n\n\nTrack improvements over time:\n# Create analysis baseline\nuv run python -m veritascribe analyze thesis.pdf --output ./baseline\ngit add baseline/\ngit commit -m \"Baseline thesis analysis\"\n\n# After revisions\nuv run python -m veritascribe analyze thesis_v2.pdf --output ./revision_1\ngit add revision_1/\ngit commit -m \"First revision analysis\"\n\n# Compare improvements\npython compare_analyses.py baseline/ revision_1/\n\n\n\n\n\n\n\nUse text-based PDFs: Avoid scanned documents when possible\nEnsure proper formatting: Well-structured documents analyze better\nCheck PDF integrity: Corrupted files may cause issues\nRemove passwords: Encrypted PDFs cannot be processed\n\n\n\n\n\nStart with quick analysis: Get overview before full analysis\nFocus on high-priority issues: Address critical errors first\nUse appropriate citation style: Match your academic field\nConsider document size: Large documents may need configuration adjustment\n\n\n\n\n\nMonitor token usage: Check costs regularly with large documents\nUse quick mode for drafts: Full analysis for final versions\nAdjust block size: Smaller blocks = more requests but finer analysis\nConsider model selection: Balance quality vs. cost\n\n\n\n\n\nRegular analysis: Integrate into writing routine\nVersion tracking: Keep analysis results with document versions\nCollaborative review: Share analysis results with advisors\nFinal validation: Run full analysis before submission\n\n\nFor troubleshooting common issues, see the Troubleshooting Guide."
  },
  {
    "objectID": "usage.html#command-overview",
    "href": "usage.html#command-overview",
    "title": "Usage Guide",
    "section": "",
    "text": "VeritaScribe provides several commands for different use cases:\n\n\n\nCommand\nPurpose\nUse Case\n\n\n\n\ndemo\nCreate and analyze sample document\nTesting setup\n\n\nquick\nFast analysis of document subset\nQuick feedback\n\n\nanalyze\nFull document analysis\nComplete review\n\n\nconfig\nView configuration\nTroubleshooting\n\n\ntest\nSystem diagnostics\nVerify installation"
  },
  {
    "objectID": "usage.html#getting-started",
    "href": "usage.html#getting-started",
    "title": "Usage Guide",
    "section": "",
    "text": "Start with the demo to familiarize yourself with VeritaScribe:\nuv run python -m veritascribe demo\nThis command will: - Create a sample thesis PDF (demo_thesis.pdf) - Perform quick analysis if API key is configured - Show example output and reports\n\n\n\nFor rapid feedback on your document:\nuv run python -m veritascribe quick your_thesis.pdf\nQuick analysis: - Analyzes first 5 text blocks by default - Provides immediate feedback - Useful during writing process - Lower API costs\nCustomize block count:\nuv run python -m veritascribe quick your_thesis.pdf --blocks 10\n\n\n\nFor comprehensive document review:\nuv run python -m veritascribe analyze your_thesis.pdf\nThis performs: - Complete document analysis - All error types detection - Detailed reporting - Visualization generation"
  },
  {
    "objectID": "usage.html#command-details",
    "href": "usage.html#command-details",
    "title": "Usage Guide",
    "section": "",
    "text": "The primary command for comprehensive thesis analysis.\n\n\nuv run python -m veritascribe analyze thesis.pdf\n\n\n\nuv run python -m veritascribe analyze thesis.pdf \\\n  --output ./results \\\n  --citation-style APA \\\n  --verbose\n\n\n\n\n\n\nOption\nShort\nDescription\nDefault\n\n\n\n\n--output\n-o\nOutput directory\n./analysis_output\n\n\n--citation-style\n-c\nCitation style\nAPA\n\n\n--quick\n-q\nQuick mode (10 blocks)\nfalse\n\n\n--no-viz\n\nSkip visualizations\nfalse\n\n\n--verbose\n-v\nVerbose logging\nfalse\n\n\n\n\n\n\n# American Psychological Association\n--citation-style APA\n\n# Modern Language Association  \n--citation-style MLA\n\n# Chicago Manual of Style\n--citation-style Chicago\n\n# IEEE format\n--citation-style IEEE\n\n# Harvard referencing\n--citation-style Harvard\n\n\n\nStandard Analysis:\nuv run python -m veritascribe analyze thesis.pdf\nCustom Output Location:\nuv run python -m veritascribe analyze thesis.pdf \\\n  --output ~/Documents/thesis_review\nMLA Citation Style:\nuv run python -m veritascribe analyze thesis.pdf \\\n  --citation-style MLA\nQuick Full Analysis:\nuv run python -m veritascribe analyze thesis.pdf --quick\nAnalysis Without Visualizations:\nuv run python -m veritascribe analyze thesis.pdf --no-viz\n\n\n\n\nIdeal for iterative writing and quick feedback.\n\n\nuv run python -m veritascribe quick thesis.pdf\n\n\n\n# Analyze first 3 blocks\nuv run python -m veritascribe quick thesis.pdf --blocks 3\n\n# Analyze first 15 blocks  \nuv run python -m veritascribe quick thesis.pdf --blocks 15\n\n\n\n\nDuring writing: Get feedback on recent sections\nInitial review: Spot-check document quality\nCost management: Reduce API usage for large documents\nDebugging: Test configuration with small sample"
  },
  {
    "objectID": "usage.html#understanding-output",
    "href": "usage.html#understanding-output",
    "title": "Usage Guide",
    "section": "",
    "text": "VeritaScribe provides rich console output with progress indicators and summaries:\nStarting analysis of: thesis.pdf\nOutput directory: ./analysis_output\n\nAnalyzing document... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100%\n\nAnalysis Results: thesis.pdf\n┌─────────────────────────────────────────────────────────────────────────────────┐\n│ 📄 Pages: 45                                                                    │\n│ 📝 Words: 12,543                                                               │\n│ 🔍 Text blocks analyzed: 87                                                     │\n│ ⚠️  Total errors: 23                                                            │\n│ 📊 Error rate: 1.83 per 1,000 words                                           │\n│ ⏱️  Processing time: 45.32s                                                     │\n└─────────────────────────────────────────────────────────────────────────────────┘\n\nErrors by Type\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Type                                                                             ┃ Count                                                                            ┃ Percentage                                                                       ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ Grammar                                                                          │ 12                                                                               │ 52.2%                                                                           │\n│ Citation Format                                                                  │ 7                                                                                │ 30.4%                                                                           │\n│ Content Plausibility                                                             │ 4                                                                                │ 17.4%                                                                           │\n└──────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────┘\n\nSeverity Breakdown: 🔴 High: 3 | 🟡 Medium: 8 | 🟢 Low: 12\n\n🚨 3 high-priority issues require immediate attention!\n\n✓ Analysis completed successfully!\n\nGenerated files:\n  📄 Text report: ./analysis_output/thesis_20240315_143022_report.md\n  📊 JSON data: ./analysis_output/thesis_20240315_143022_data.json\n  📈 Visualizations: ./analysis_output/thesis_20240315_143022_visualizations/\n\nRecommendation: Document shows good overall quality with some areas for improvement. Focus on addressing high-priority grammar and citation issues first.\n\n\n\nEach analysis produces several output files:\n\n\nComprehensive Markdown report with:\n# VeritaScribe Analysis Report\n\n**Document:** thesis.pdf\n**Analysis Date:** 2024-03-15 14:30:22\n**Processing Time:** 45.32 seconds\n\n## Executive Summary\n\n- **Total Pages:** 45\n- **Word Count:** 12,543\n- **Text Blocks Analyzed:** 87\n- **Total Errors Found:** 23\n- **Error Rate:** 1.83 per 1,000 words\n\n## Error Summary\n\n### By Type\n- Grammar: 12 errors (52.2%)\n- Citation Format: 7 errors (30.4%)  \n- Content Plausibility: 4 errors (17.4%)\n\n### By Severity\n- High Priority: 3 errors\n- Medium Priority: 8 errors\n- Low Priority: 12 errors\n\n## Detailed Findings\n\n### High Priority Issues\n\n#### 1. Grammar Error (Page 12)\n**Location:** Page 12, coordinates (72, 345, 520, 365)\n**Original Text:** \"The results shows that...\"\n**Suggested Fix:** \"The results show that...\"\n**Explanation:** Subject-verb agreement error. 'Results' is plural and requires 'show' not 'shows'.\n\n#### 2. Citation Format Error (Page 23)\n**Location:** Page 23, coordinates (72, 123, 520, 143)\n**Original Text:** \"(Smith 2020)\"\n**Suggested Fix:** \"(Smith, 2020)\"\n**Explanation:** APA format requires comma between author and year.\n\n[... more detailed errors ...]\n\n## Recommendations\n\n1. **Immediate Action Required:** Address 3 high-priority issues\n2. **Grammar Focus:** Review subject-verb agreements throughout\n3. **Citation Consistency:** Ensure all citations follow APA format\n4. **Content Review:** Verify factual claims in identified sections\n\n## Overall Assessment\n\nDocument quality: **Good** with room for improvement.\nRecommended next steps: Focus on high-priority issues first.\n\n\n\nStructured data for programmatic access:\n{\n  \"document_name\": \"thesis.pdf\",\n  \"analysis_timestamp\": \"2024-03-15T14:30:22.123456\",\n  \"total_pages\": 45,\n  \"total_words\": 12543,\n  \"total_text_blocks\": 87,\n  \"total_errors\": 23,\n  \"error_rate\": 1.83,\n  \"total_processing_time_seconds\": 45.32,\n  \"errors_by_type\": {\n    \"grammar\": 12,\n    \"citation_format\": 7,\n    \"content_plausibility\": 4\n  },\n  \"errors_by_severity\": {\n    \"high\": 3,\n    \"medium\": 8,\n    \"low\": 12\n  },\n  \"analysis_results\": [\n    {\n      \"text_block\": {\n        \"content\": \"The results shows that...\",\n        \"page_number\": 12,\n        \"bounding_box\": [72, 345, 520, 365]\n      },\n      \"errors\": [\n        {\n          \"error_type\": \"grammar\",\n          \"severity\": \"high\",\n          \"original_text\": \"The results shows that\",\n          \"suggested_correction\": \"The results show that\",\n          \"explanation\": \"Subject-verb agreement error...\",\n          \"location\": {\n            \"page_number\": 12,\n            \"bounding_box\": [72, 345, 520, 365]\n          }\n        }\n      ]\n    }\n  ]\n}\n\n\n\nCharts and graphs in the visualizations directory:\n\nError Distribution by Type (error_types.png)\nError Density by Page (error_density.png)\nSeverity Breakdown (severity_breakdown.png)"
  },
  {
    "objectID": "usage.html#advanced-usage-patterns",
    "href": "usage.html#advanced-usage-patterns",
    "title": "Usage Guide",
    "section": "",
    "text": "Process multiple documents:\n# Create script for batch processing\ncat &gt; batch_analyze.sh &lt;&lt; 'EOF'\n#!/bin/bash\nfor pdf in *.pdf; do\n  echo \"Analyzing $pdf...\"\n  uv run python -m veritascribe analyze \"$pdf\" \\\n    --output \"./results/$(basename \"$pdf\" .pdf)\"\ndone\nEOF\n\nchmod +x batch_analyze.sh\n./batch_analyze.sh\n\n\n\nDifferent workflows for different academic styles:\n# APA Style (Psychology, Education)\nuv run python -m veritascribe analyze thesis.pdf --citation-style APA\n\n# MLA Style (Literature, Humanities)  \nuv run python -m veritascribe analyze thesis.pdf --citation-style MLA\n\n# Chicago Style (History, Arts)\nuv run python -m veritascribe analyze thesis.pdf --citation-style Chicago\n\n# IEEE Style (Engineering, Computer Science)\nuv run python -m veritascribe analyze thesis.pdf --citation-style IEEE\n\n\n\nWorkflow for document improvement:\n# Step 1: Initial quick review\nuv run python -m veritascribe quick draft.pdf --blocks 10\n\n# Step 2: Address major issues, then full analysis\nuv run python -m veritascribe analyze draft.pdf --output ./review_1\n\n# Step 3: After revisions, re-analyze\nuv run python -m veritascribe analyze revised_draft.pdf --output ./review_2\n\n# Step 4: Compare results\ndiff ./review_1/draft_*_data.json ./review_2/revised_draft_*_data.json\n\n\n\nOptimize API usage for large documents:\n# Strategy 1: Quick analysis first\nuv run python -m veritascribe quick large_thesis.pdf --blocks 20\n\n# Strategy 2: Disable expensive analysis types\nCONTENT_ANALYSIS_ENABLED=false \\\nuv run python -m veritascribe analyze large_thesis.pdf\n\n# Strategy 3: Use cheaper model\nDEFAULT_MODEL=gpt-3.5-turbo \\\nuv run python -m veritascribe analyze large_thesis.pdf\n\n# Strategy 4: Reduce parallel processing\nMAX_CONCURRENT_REQUESTS=2 \\\nuv run python -m veritascribe analyze large_thesis.pdf"
  },
  {
    "objectID": "usage.html#working-with-results",
    "href": "usage.html#working-with-results",
    "title": "Usage Guide",
    "section": "",
    "text": "High Priority (Score ≥ 0.8): - Critical grammar errors - Major citation format violations - Significant factual inconsistencies - Action: Fix immediately\nMedium Priority (Score ≥ 0.5): - Minor grammar issues - Style inconsistencies - Citation formatting preferences - Action: Review and fix as time permits\nLow Priority (Score &lt; 0.5): - Style suggestions - Minor formatting preferences - Optional improvements - Action: Consider for polish pass\n\n\n\nEach error includes precise location data:\n{\n  \"location\": {\n    \"page_number\": 12,\n    \"bounding_box\": [72, 345, 520, 365]\n  }\n}\nBounding box coordinates [x1, y1, x2, y2]: - (x1, y1): Top-left corner - (x2, y2): Bottom-right corner - Units are in PDF points (72 points = 1 inch)\n\n\n\nProcess results programmatically:\nimport json\n\n# Load analysis results\nwith open('thesis_20240315_143022_data.json', 'r') as f:\n    results = json.load(f)\n\n# Get high-priority errors\nhigh_priority = [\n    error for result in results['analysis_results']\n    for error in result['errors']\n    if error['severity'] == 'high'\n]\n\nprint(f\"Found {len(high_priority)} high-priority issues\")\n\n# Group errors by type\nerror_types = {}\nfor result in results['analysis_results']:\n    for error in result['errors']:\n        error_type = error['error_type']\n        error_types[error_type] = error_types.get(error_type, 0) + 1\n\nprint(\"Error distribution:\", error_types)"
  },
  {
    "objectID": "usage.html#integration-with-other-tools",
    "href": "usage.html#integration-with-other-tools",
    "title": "Usage Guide",
    "section": "",
    "text": "Import location information into editors:\n# Generate editor jump commands\ndef generate_editor_commands(json_file):\n    with open(json_file, 'r') as f:\n        results = json.load(f)\n    \n    commands = []\n    for result in results['analysis_results']:\n        for error in result['errors']:\n            page = error['location']['page_number']\n            text = error['original_text'][:50]\n            commands.append(f\"# Page {page}: {text}...\")\n    \n    return commands\n\ncommands = generate_editor_commands('results.json')\nfor cmd in commands[:5]:  # Show first 5\n    print(cmd)\n\n\n\nExport citation issues for reference managers:\ndef export_citation_issues(json_file):\n    with open(json_file, 'r') as f:\n        results = json.load(f)\n    \n    citation_errors = []\n    for result in results['analysis_results']:\n        for error in result['errors']:\n            if error['error_type'] == 'citation_format':\n                citation_errors.append({\n                    'page': error['location']['page_number'],\n                    'original': error['original_text'],\n                    'suggested': error['suggested_correction'],\n                    'explanation': error['explanation']\n                })\n    \n    return citation_errors\n\n\n\nTrack improvements over time:\n# Create analysis baseline\nuv run python -m veritascribe analyze thesis.pdf --output ./baseline\ngit add baseline/\ngit commit -m \"Baseline thesis analysis\"\n\n# After revisions\nuv run python -m veritascribe analyze thesis_v2.pdf --output ./revision_1\ngit add revision_1/\ngit commit -m \"First revision analysis\"\n\n# Compare improvements\npython compare_analyses.py baseline/ revision_1/"
  },
  {
    "objectID": "usage.html#best-practices",
    "href": "usage.html#best-practices",
    "title": "Usage Guide",
    "section": "",
    "text": "Use text-based PDFs: Avoid scanned documents when possible\nEnsure proper formatting: Well-structured documents analyze better\nCheck PDF integrity: Corrupted files may cause issues\nRemove passwords: Encrypted PDFs cannot be processed\n\n\n\n\n\nStart with quick analysis: Get overview before full analysis\nFocus on high-priority issues: Address critical errors first\nUse appropriate citation style: Match your academic field\nConsider document size: Large documents may need configuration adjustment\n\n\n\n\n\nMonitor token usage: Check costs regularly with large documents\nUse quick mode for drafts: Full analysis for final versions\nAdjust block size: Smaller blocks = more requests but finer analysis\nConsider model selection: Balance quality vs. cost\n\n\n\n\n\nRegular analysis: Integrate into writing routine\nVersion tracking: Keep analysis results with document versions\nCollaborative review: Share analysis results with advisors\nFinal validation: Run full analysis before submission\n\n\nFor troubleshooting common issues, see the Troubleshooting Guide."
  },
  {
    "objectID": "troubleshooting.html",
    "href": "troubleshooting.html",
    "title": "Troubleshooting Guide",
    "section": "",
    "text": "This guide helps you diagnose and resolve common issues with VeritaScribe.\n\n\nBefore diving into specific issues, run these diagnostic commands:\n# Check system status\nuv run python -m veritascribe test\n\n# View current configuration\nuv run python -m veritascribe config\n\n# Try demo analysis\nuv run python -m veritascribe demo\n\n\n\n\n\nProblem: uv: command not found\nSolutions:\n\nInstall uv:\n# macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Windows PowerShell\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Alternative: pip install\npip install uv\nRestart terminal after installation\nCheck PATH:\necho $PATH | grep -o \"[^:]*uv[^:]*\"\n\n\n\n\nProblem: Python 3.13+ required but found 3.x.x\nSolutions:\n\nCheck available Python versions:\npython --version\npython3 --version\npython3.13 --version\nInstall Python 3.13:\n# macOS with Homebrew\nbrew install python@3.13\n\n# Ubuntu/Debian\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt update && sudo apt install python3.13\nUse specific Python version:\nuv python install 3.13\nuv venv --python 3.13\n\n\n\n\nProblem: uv sync fails with compilation errors\nSolutions:\n\nUpdate uv:\nuv self update\nClear cache:\nuv cache clean\nInstall system dependencies:\n# macOS\nxcode-select --install\n\n# Ubuntu/Debian\nsudo apt update\nsudo apt install build-essential python3-dev\n\n# CentOS/RHEL\nsudo yum groupinstall \"Development Tools\"\nsudo yum install python3-devel\nUse pre-compiled wheels:\nuv sync --only-binary=all\n\n\n\n\nProblem: Permission errors during installation\nSolutions:\n\nDon’t use sudo with uv:\n# Wrong\nsudo uv sync\n\n# Correct\nuv sync\nFix directory permissions:\n# macOS/Linux\nsudo chown -R $(whoami) ~/.local/share/uv\nUse virtual environment:\nuv venv venv\nsource venv/bin/activate  # Linux/macOS\n# or\nvenv\\Scripts\\activate     # Windows\n\n\n\n\n\n\n\nProblem: OpenAI API key is required for analysis\nDiagnosis:\n# Check if .env file exists\nls -la .env\n\n# Check environment variables\necho $OPENAI_API_KEY\n\n# Test API key validity\nuv run python -c \"\nfrom openai import OpenAI\ntry:\n    client = OpenAI()\n    client.models.list()\n    print('✓ API key is valid')\nexcept Exception as e:\n    print(f'✗ API key error: {e}')\n\"\nSolutions:\n\nCreate .env file:\ncp .env.example .env\n# Edit .env and add your API key\nVerify API key format:\n\nShould start with sk-\nShould be 51+ characters long\nNo extra spaces or quotes\n\nSet environment variable directly:\nexport OPENAI_API_KEY=\"your-key-here\"\nuv run python -m veritascribe config\nCheck API key permissions:\n\nVisit OpenAI Platform\nVerify key has sufficient permissions\nCheck billing/credit status\n\n\n\n\n\nProblem: Model 'gpt-4' not available\nDiagnosis:\n# Check available models\nuv run python -c \"\nfrom openai import OpenAI\nclient = OpenAI()\nmodels = client.models.list()\nfor model in models.data:\n    if 'gpt' in model.id:\n        print(model.id)\n\"\nSolutions:\n\nUse available model:\n# Try GPT-3.5 Turbo\nDEFAULT_MODEL=gpt-3.5-turbo uv run python -m veritascribe demo\nCheck API tier:\n\nVisit OpenAI Usage\nVerify your tier supports GPT-4\nConsider upgrading if needed\n\nUpdate configuration:\n# In .env file\nDEFAULT_MODEL=gpt-3.5-turbo\n\n\n\n\nProblem: Failed to load configuration\nDiagnosis:\n# Check .env file format\ncat .env | grep -E \"^[A-Z_]+=.*$\"\n\n# Validate configuration\nuv run python -c \"\nfrom veritascribe.config import load_settings\ntry:\n    settings = load_settings()\n    print('✓ Configuration valid')\nexcept Exception as e:\n    print(f'✗ Configuration error: {e}')\n\"\nSolutions:\n\nFix .env format:\n# Correct format\nOPENAI_API_KEY=sk-your-key-here\nDEFAULT_MODEL=gpt-4\n\n# Wrong format (quotes, spaces)\nOPENAI_API_KEY = \"sk-your-key-here\"\nCheck file encoding:\nfile .env\n# Should show UTF-8 encoding\nReset to defaults:\ncp .env.example .env\n# Edit with minimal required settings\n\n\n\n\n\n\n\nProblem: Error: PDF file not found\nSolutions:\n\nCheck file path:\n# Use absolute path\nuv run python -m veritascribe analyze /full/path/to/thesis.pdf\n\n# Or relative from current directory\nls -la *.pdf\nuv run python -m veritascribe analyze ./thesis.pdf\nVerify file permissions:\nls -la thesis.pdf\n# Should show read permissions\nCheck file extension:\nfile thesis.pdf\n# Should show \"PDF document\"\n\n\n\n\nProblem: No text blocks extracted or PDF processing failed\nDiagnosis:\n# Test PDF with simple extraction\nuv run python -c \"\nimport fitz\ntry:\n    doc = fitz.open('thesis.pdf')\n    text = doc[0].get_text()\n    print(f'✓ Extracted {len(text)} characters from first page')\n    doc.close()\nexcept Exception as e:\n    print(f'✗ PDF error: {e}')\n\"\nSolutions:\n\nCheck PDF type:\n# Text-based PDFs work best\npdfinfo thesis.pdf | grep -E \"(Pages|Producer|Creator)\"\nTry different PDF:\n# Test with demo PDF\nuv run python -m veritascribe demo\nHandle password-protected PDFs:\n# Remove password first\nqpdf --password=PASSWORD --decrypt input.pdf output.pdf\nConvert scanned PDFs:\n# Use OCR tools first\nocrmypdf input.pdf output.pdf\n\n\n\n\nProblem: Out of memory or slow processing\nSolutions:\n\nReduce block size:\nMAX_TEXT_BLOCK_SIZE=1000 uv run python -m veritascribe analyze large.pdf\nDisable parallel processing:\nPARALLEL_PROCESSING=false uv run python -m veritascribe analyze large.pdf\nUse quick analysis:\nuv run python -m veritascribe quick large.pdf --blocks 20\nSplit large documents:\n# Split PDF into smaller parts\npdftk input.pdf burst output page_%02d.pdf\n\n\n\n\n\n\n\nProblem: Analysis modules failed or timeout errors\nDiagnosis:\n# Test LLM connectivity\nuv run python -c \"\nimport dspy\nfrom veritascribe.config import get_dspy_config\n\ntry:\n    config = get_dspy_config()\n    lm = config.initialize_llm()\n    response = lm('Test prompt')\n    print('✓ LLM connection working')\nexcept Exception as e:\n    print(f'✗ LLM error: {e}')\n\"\nSolutions:\n\nReduce concurrency:\nMAX_CONCURRENT_REQUESTS=2 uv run python -m veritascribe analyze thesis.pdf\nIncrease timeout/retries:\nMAX_RETRIES=5 RETRY_DELAY=2.0 uv run python -m veritascribe analyze thesis.pdf\nCheck rate limits:\n\nVisit OpenAI Usage\nVerify you haven’t hit rate limits\nConsider upgrading API tier\n\nUse simpler model:\nDEFAULT_MODEL=gpt-3.5-turbo uv run python -m veritascribe analyze thesis.pdf\n\n\n\n\nProblem: JSON parsing error or invalid responses\nSolutions:\n\nEnable verbose logging:\nuv run python -m veritascribe analyze thesis.pdf --verbose\nReduce temperature:\nTEMPERATURE=0.0 uv run python -m veritascribe analyze thesis.pdf\nCheck token limits:\nMAX_TOKENS=1500 uv run python -m veritascribe analyze thesis.pdf\n\n\n\n\nProblem: Unexpected high token usage\nSolutions:\n\nMonitor usage:\n# Check configuration\nuv run python -m veritascribe config\nOptimize settings:\n# Cost-optimized configuration\nDEFAULT_MODEL=gpt-3.5-turbo\nMAX_TOKENS=1500\nMAX_TEXT_BLOCK_SIZE=1000\nPARALLEL_PROCESSING=false\nUse quick analysis:\nuv run python -m veritascribe quick thesis.pdf --blocks 10\nDisable analysis types:\nCONTENT_ANALYSIS_ENABLED=false uv run python -m veritascribe analyze thesis.pdf\n\n\n\n\n\n\n\nProblem: Report generation failed or missing output files\nSolutions:\n\nCheck output directory permissions:\nmkdir -p ./analysis_output\nchmod 755 ./analysis_output\nSpecify output directory:\nuv run python -m veritascribe analyze thesis.pdf --output ~/Documents/analysis\nDisable problematic outputs:\n# Skip visualizations if matplotlib issues\nuv run python -m veritascribe analyze thesis.pdf --no-viz\n\n\n\n\nProblem: Chart generation fails\nSolutions:\n\nInstall GUI backend:\n# macOS\nbrew install python-tk\n\n# Ubuntu/Debian\nsudo apt install python3-tk\nUse headless backend:\nuv run python -c \"\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nprint('✓ Matplotlib working')\n\"\nSkip visualizations:\nGENERATE_VISUALIZATIONS=false uv run python -m veritascribe analyze thesis.pdf\n\n\n\n\n\n\n\nProblem: Analysis takes too long\nDiagnosis:\n# Profile analysis\ntime uv run python -m veritascribe quick thesis.pdf --blocks 5\nSolutions:\n\nEnable parallel processing:\nPARALLEL_PROCESSING=true MAX_CONCURRENT_REQUESTS=5\nUse faster model:\nDEFAULT_MODEL=gpt-3.5-turbo\nReduce analysis scope:\n# Disable expensive analysis\nCONTENT_ANALYSIS_ENABLED=false\nOptimize block size:\nMAX_TEXT_BLOCK_SIZE=1500\n\n\n\n\nProblem: High memory consumption\nSolutions:\n\nMonitor memory:\n# Use memory profiler\npip install memory-profiler\nmprof run uv run python -m veritascribe analyze thesis.pdf\nmprof plot\nReduce batch size:\nMAX_CONCURRENT_REQUESTS=2\nClear cache:\n# Clear Python cache\nfind . -name \"*.pyc\" -delete\nfind . -name \"__pycache__\" -delete\n\n\n\n\n\n\n\nProblem: Connection timeout or network errors\nSolutions:\n\nCheck internet connectivity:\ncurl -I https://api.openai.com/v1/models\nConfigure proxy (if needed):\nexport HTTPS_PROXY=http://proxy.company.com:8080\nexport HTTP_PROXY=http://proxy.company.com:8080\nIncrease timeout:\n# Configure longer timeouts in requests\nREQUESTS_TIMEOUT=60\n\n\n\n\nProblem: Requests blocked by firewall\nSolutions:\n\nWhitelist OpenAI domains:\n\napi.openai.com\nopenai.com\n\nCheck corporate policies:\n\nContact IT about OpenAI API access\nConsider VPN if needed\n\nTest with curl:\ncurl -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n     https://api.openai.com/v1/models\n\n\n\n\n\n\n\nFor any issue, enable verbose logging:\n# Enable debug output\nuv run python -m veritascribe analyze thesis.pdf --verbose\n\n# Python logging\nPYTHONPATH=. python -c \"\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\nfrom veritascribe.main import main\nmain()\n\"\n\n\n\n# System info script\ncat &gt; debug_info.sh &lt;&lt; 'EOF'\n#!/bin/bash\necho \"=== System Information ===\"\nuname -a\npython --version\nuv --version\n\necho -e \"\\n=== VeritaScribe Configuration ===\"\nuv run python -m veritascribe config\n\necho -e \"\\n=== Environment Variables ===\"\nenv | grep -E \"(OPENAI|PYTHON|UV)\" | sort\n\necho -e \"\\n=== System Tests ===\"\nuv run python -m veritascribe test\n\necho -e \"\\n=== Dependencies ===\"\nuv tree\nEOF\n\nchmod +x debug_info.sh\n./debug_info.sh &gt; debug_info.txt\n\n\n\n# Create minimal test case\ncat &gt; minimal_test.py &lt;&lt; 'EOF'\n#!/usr/bin/env python3\n\"\"\"Minimal reproduction script.\"\"\"\n\nfrom veritascribe.config import load_settings\nfrom veritascribe.pdf_processor import PDFProcessor\nfrom veritascribe.pipeline import create_quick_pipeline\n\ndef main():\n    try:\n        # Test configuration\n        print(\"Testing configuration...\")\n        settings = load_settings()\n        print(f\"✓ Config loaded, model: {settings.default_model}\")\n        \n        # Test PDF processing\n        print(\"Testing PDF processing...\")\n        processor = PDFProcessor()\n        # Add your test PDF here\n        \n        # Test analysis\n        print(\"Testing analysis...\")\n        pipeline = create_quick_pipeline()\n        # Add your test case here\n        \n        print(\"✓ All tests passed\")\n        \n    except Exception as e:\n        print(f\"✗ Error: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()\nEOF\n\nuv run python minimal_test.py\n\n\n\n\n\n\nError Message\nLikely Cause\nQuick Fix\n\n\n\n\nuv: command not found\nuv not installed\nInstall uv\n\n\nOpenAI API key is required\nMissing API key\nSet OPENAI_API_KEY\n\n\nPDF file not found\nWrong file path\nCheck file path\n\n\nNo text blocks extracted\nScanned PDF\nUse OCR first\n\n\nConnection timeout\nNetwork issue\nCheck connectivity\n\n\nRate limit exceeded\nToo many requests\nReduce concurrency\n\n\nModel not available\nWrong model name\nUse gpt-3.5-turbo\n\n\nJSON parsing error\nMalformed LLM response\nReduce temperature\n\n\nPermission denied\nFile permissions\nCheck file access\n\n\nOut of memory\nLarge document\nReduce block size\n\n\n\n\n\n\nCreate an issue on the project repository with:\n\nError message and full traceback\nSystem information from debug script\nMinimal reproduction case\nSteps taken to resolve the issue\nExpected vs. actual behavior\n\n\nIf none of these solutions work, please create an issue with detailed information about your environment and the specific error you’re encountering."
  },
  {
    "objectID": "troubleshooting.html#quick-diagnostic-commands",
    "href": "troubleshooting.html#quick-diagnostic-commands",
    "title": "Troubleshooting Guide",
    "section": "",
    "text": "Before diving into specific issues, run these diagnostic commands:\n# Check system status\nuv run python -m veritascribe test\n\n# View current configuration\nuv run python -m veritascribe config\n\n# Try demo analysis\nuv run python -m veritascribe demo"
  },
  {
    "objectID": "troubleshooting.html#installation-issues",
    "href": "troubleshooting.html#installation-issues",
    "title": "Troubleshooting Guide",
    "section": "",
    "text": "Problem: uv: command not found\nSolutions:\n\nInstall uv:\n# macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Windows PowerShell\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Alternative: pip install\npip install uv\nRestart terminal after installation\nCheck PATH:\necho $PATH | grep -o \"[^:]*uv[^:]*\"\n\n\n\n\nProblem: Python 3.13+ required but found 3.x.x\nSolutions:\n\nCheck available Python versions:\npython --version\npython3 --version\npython3.13 --version\nInstall Python 3.13:\n# macOS with Homebrew\nbrew install python@3.13\n\n# Ubuntu/Debian\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt update && sudo apt install python3.13\nUse specific Python version:\nuv python install 3.13\nuv venv --python 3.13\n\n\n\n\nProblem: uv sync fails with compilation errors\nSolutions:\n\nUpdate uv:\nuv self update\nClear cache:\nuv cache clean\nInstall system dependencies:\n# macOS\nxcode-select --install\n\n# Ubuntu/Debian\nsudo apt update\nsudo apt install build-essential python3-dev\n\n# CentOS/RHEL\nsudo yum groupinstall \"Development Tools\"\nsudo yum install python3-devel\nUse pre-compiled wheels:\nuv sync --only-binary=all\n\n\n\n\nProblem: Permission errors during installation\nSolutions:\n\nDon’t use sudo with uv:\n# Wrong\nsudo uv sync\n\n# Correct\nuv sync\nFix directory permissions:\n# macOS/Linux\nsudo chown -R $(whoami) ~/.local/share/uv\nUse virtual environment:\nuv venv venv\nsource venv/bin/activate  # Linux/macOS\n# or\nvenv\\Scripts\\activate     # Windows"
  },
  {
    "objectID": "troubleshooting.html#configuration-issues",
    "href": "troubleshooting.html#configuration-issues",
    "title": "Troubleshooting Guide",
    "section": "",
    "text": "Problem: OpenAI API key is required for analysis\nDiagnosis:\n# Check if .env file exists\nls -la .env\n\n# Check environment variables\necho $OPENAI_API_KEY\n\n# Test API key validity\nuv run python -c \"\nfrom openai import OpenAI\ntry:\n    client = OpenAI()\n    client.models.list()\n    print('✓ API key is valid')\nexcept Exception as e:\n    print(f'✗ API key error: {e}')\n\"\nSolutions:\n\nCreate .env file:\ncp .env.example .env\n# Edit .env and add your API key\nVerify API key format:\n\nShould start with sk-\nShould be 51+ characters long\nNo extra spaces or quotes\n\nSet environment variable directly:\nexport OPENAI_API_KEY=\"your-key-here\"\nuv run python -m veritascribe config\nCheck API key permissions:\n\nVisit OpenAI Platform\nVerify key has sufficient permissions\nCheck billing/credit status\n\n\n\n\n\nProblem: Model 'gpt-4' not available\nDiagnosis:\n# Check available models\nuv run python -c \"\nfrom openai import OpenAI\nclient = OpenAI()\nmodels = client.models.list()\nfor model in models.data:\n    if 'gpt' in model.id:\n        print(model.id)\n\"\nSolutions:\n\nUse available model:\n# Try GPT-3.5 Turbo\nDEFAULT_MODEL=gpt-3.5-turbo uv run python -m veritascribe demo\nCheck API tier:\n\nVisit OpenAI Usage\nVerify your tier supports GPT-4\nConsider upgrading if needed\n\nUpdate configuration:\n# In .env file\nDEFAULT_MODEL=gpt-3.5-turbo\n\n\n\n\nProblem: Failed to load configuration\nDiagnosis:\n# Check .env file format\ncat .env | grep -E \"^[A-Z_]+=.*$\"\n\n# Validate configuration\nuv run python -c \"\nfrom veritascribe.config import load_settings\ntry:\n    settings = load_settings()\n    print('✓ Configuration valid')\nexcept Exception as e:\n    print(f'✗ Configuration error: {e}')\n\"\nSolutions:\n\nFix .env format:\n# Correct format\nOPENAI_API_KEY=sk-your-key-here\nDEFAULT_MODEL=gpt-4\n\n# Wrong format (quotes, spaces)\nOPENAI_API_KEY = \"sk-your-key-here\"\nCheck file encoding:\nfile .env\n# Should show UTF-8 encoding\nReset to defaults:\ncp .env.example .env\n# Edit with minimal required settings"
  },
  {
    "objectID": "troubleshooting.html#pdf-processing-issues",
    "href": "troubleshooting.html#pdf-processing-issues",
    "title": "Troubleshooting Guide",
    "section": "",
    "text": "Problem: Error: PDF file not found\nSolutions:\n\nCheck file path:\n# Use absolute path\nuv run python -m veritascribe analyze /full/path/to/thesis.pdf\n\n# Or relative from current directory\nls -la *.pdf\nuv run python -m veritascribe analyze ./thesis.pdf\nVerify file permissions:\nls -la thesis.pdf\n# Should show read permissions\nCheck file extension:\nfile thesis.pdf\n# Should show \"PDF document\"\n\n\n\n\nProblem: No text blocks extracted or PDF processing failed\nDiagnosis:\n# Test PDF with simple extraction\nuv run python -c \"\nimport fitz\ntry:\n    doc = fitz.open('thesis.pdf')\n    text = doc[0].get_text()\n    print(f'✓ Extracted {len(text)} characters from first page')\n    doc.close()\nexcept Exception as e:\n    print(f'✗ PDF error: {e}')\n\"\nSolutions:\n\nCheck PDF type:\n# Text-based PDFs work best\npdfinfo thesis.pdf | grep -E \"(Pages|Producer|Creator)\"\nTry different PDF:\n# Test with demo PDF\nuv run python -m veritascribe demo\nHandle password-protected PDFs:\n# Remove password first\nqpdf --password=PASSWORD --decrypt input.pdf output.pdf\nConvert scanned PDFs:\n# Use OCR tools first\nocrmypdf input.pdf output.pdf\n\n\n\n\nProblem: Out of memory or slow processing\nSolutions:\n\nReduce block size:\nMAX_TEXT_BLOCK_SIZE=1000 uv run python -m veritascribe analyze large.pdf\nDisable parallel processing:\nPARALLEL_PROCESSING=false uv run python -m veritascribe analyze large.pdf\nUse quick analysis:\nuv run python -m veritascribe quick large.pdf --blocks 20\nSplit large documents:\n# Split PDF into smaller parts\npdftk input.pdf burst output page_%02d.pdf"
  },
  {
    "objectID": "troubleshooting.html#analysis-issues",
    "href": "troubleshooting.html#analysis-issues",
    "title": "Troubleshooting Guide",
    "section": "",
    "text": "Problem: Analysis modules failed or timeout errors\nDiagnosis:\n# Test LLM connectivity\nuv run python -c \"\nimport dspy\nfrom veritascribe.config import get_dspy_config\n\ntry:\n    config = get_dspy_config()\n    lm = config.initialize_llm()\n    response = lm('Test prompt')\n    print('✓ LLM connection working')\nexcept Exception as e:\n    print(f'✗ LLM error: {e}')\n\"\nSolutions:\n\nReduce concurrency:\nMAX_CONCURRENT_REQUESTS=2 uv run python -m veritascribe analyze thesis.pdf\nIncrease timeout/retries:\nMAX_RETRIES=5 RETRY_DELAY=2.0 uv run python -m veritascribe analyze thesis.pdf\nCheck rate limits:\n\nVisit OpenAI Usage\nVerify you haven’t hit rate limits\nConsider upgrading API tier\n\nUse simpler model:\nDEFAULT_MODEL=gpt-3.5-turbo uv run python -m veritascribe analyze thesis.pdf\n\n\n\n\nProblem: JSON parsing error or invalid responses\nSolutions:\n\nEnable verbose logging:\nuv run python -m veritascribe analyze thesis.pdf --verbose\nReduce temperature:\nTEMPERATURE=0.0 uv run python -m veritascribe analyze thesis.pdf\nCheck token limits:\nMAX_TOKENS=1500 uv run python -m veritascribe analyze thesis.pdf\n\n\n\n\nProblem: Unexpected high token usage\nSolutions:\n\nMonitor usage:\n# Check configuration\nuv run python -m veritascribe config\nOptimize settings:\n# Cost-optimized configuration\nDEFAULT_MODEL=gpt-3.5-turbo\nMAX_TOKENS=1500\nMAX_TEXT_BLOCK_SIZE=1000\nPARALLEL_PROCESSING=false\nUse quick analysis:\nuv run python -m veritascribe quick thesis.pdf --blocks 10\nDisable analysis types:\nCONTENT_ANALYSIS_ENABLED=false uv run python -m veritascribe analyze thesis.pdf"
  },
  {
    "objectID": "troubleshooting.html#output-issues",
    "href": "troubleshooting.html#output-issues",
    "title": "Troubleshooting Guide",
    "section": "",
    "text": "Problem: Report generation failed or missing output files\nSolutions:\n\nCheck output directory permissions:\nmkdir -p ./analysis_output\nchmod 755 ./analysis_output\nSpecify output directory:\nuv run python -m veritascribe analyze thesis.pdf --output ~/Documents/analysis\nDisable problematic outputs:\n# Skip visualizations if matplotlib issues\nuv run python -m veritascribe analyze thesis.pdf --no-viz\n\n\n\n\nProblem: Chart generation fails\nSolutions:\n\nInstall GUI backend:\n# macOS\nbrew install python-tk\n\n# Ubuntu/Debian\nsudo apt install python3-tk\nUse headless backend:\nuv run python -c \"\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nprint('✓ Matplotlib working')\n\"\nSkip visualizations:\nGENERATE_VISUALIZATIONS=false uv run python -m veritascribe analyze thesis.pdf"
  },
  {
    "objectID": "troubleshooting.html#performance-issues",
    "href": "troubleshooting.html#performance-issues",
    "title": "Troubleshooting Guide",
    "section": "",
    "text": "Problem: Analysis takes too long\nDiagnosis:\n# Profile analysis\ntime uv run python -m veritascribe quick thesis.pdf --blocks 5\nSolutions:\n\nEnable parallel processing:\nPARALLEL_PROCESSING=true MAX_CONCURRENT_REQUESTS=5\nUse faster model:\nDEFAULT_MODEL=gpt-3.5-turbo\nReduce analysis scope:\n# Disable expensive analysis\nCONTENT_ANALYSIS_ENABLED=false\nOptimize block size:\nMAX_TEXT_BLOCK_SIZE=1500\n\n\n\n\nProblem: High memory consumption\nSolutions:\n\nMonitor memory:\n# Use memory profiler\npip install memory-profiler\nmprof run uv run python -m veritascribe analyze thesis.pdf\nmprof plot\nReduce batch size:\nMAX_CONCURRENT_REQUESTS=2\nClear cache:\n# Clear Python cache\nfind . -name \"*.pyc\" -delete\nfind . -name \"__pycache__\" -delete"
  },
  {
    "objectID": "troubleshooting.html#network-issues",
    "href": "troubleshooting.html#network-issues",
    "title": "Troubleshooting Guide",
    "section": "",
    "text": "Problem: Connection timeout or network errors\nSolutions:\n\nCheck internet connectivity:\ncurl -I https://api.openai.com/v1/models\nConfigure proxy (if needed):\nexport HTTPS_PROXY=http://proxy.company.com:8080\nexport HTTP_PROXY=http://proxy.company.com:8080\nIncrease timeout:\n# Configure longer timeouts in requests\nREQUESTS_TIMEOUT=60\n\n\n\n\nProblem: Requests blocked by firewall\nSolutions:\n\nWhitelist OpenAI domains:\n\napi.openai.com\nopenai.com\n\nCheck corporate policies:\n\nContact IT about OpenAI API access\nConsider VPN if needed\n\nTest with curl:\ncurl -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n     https://api.openai.com/v1/models"
  },
  {
    "objectID": "troubleshooting.html#getting-help",
    "href": "troubleshooting.html#getting-help",
    "title": "Troubleshooting Guide",
    "section": "",
    "text": "For any issue, enable verbose logging:\n# Enable debug output\nuv run python -m veritascribe analyze thesis.pdf --verbose\n\n# Python logging\nPYTHONPATH=. python -c \"\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\nfrom veritascribe.main import main\nmain()\n\"\n\n\n\n# System info script\ncat &gt; debug_info.sh &lt;&lt; 'EOF'\n#!/bin/bash\necho \"=== System Information ===\"\nuname -a\npython --version\nuv --version\n\necho -e \"\\n=== VeritaScribe Configuration ===\"\nuv run python -m veritascribe config\n\necho -e \"\\n=== Environment Variables ===\"\nenv | grep -E \"(OPENAI|PYTHON|UV)\" | sort\n\necho -e \"\\n=== System Tests ===\"\nuv run python -m veritascribe test\n\necho -e \"\\n=== Dependencies ===\"\nuv tree\nEOF\n\nchmod +x debug_info.sh\n./debug_info.sh &gt; debug_info.txt\n\n\n\n# Create minimal test case\ncat &gt; minimal_test.py &lt;&lt; 'EOF'\n#!/usr/bin/env python3\n\"\"\"Minimal reproduction script.\"\"\"\n\nfrom veritascribe.config import load_settings\nfrom veritascribe.pdf_processor import PDFProcessor\nfrom veritascribe.pipeline import create_quick_pipeline\n\ndef main():\n    try:\n        # Test configuration\n        print(\"Testing configuration...\")\n        settings = load_settings()\n        print(f\"✓ Config loaded, model: {settings.default_model}\")\n        \n        # Test PDF processing\n        print(\"Testing PDF processing...\")\n        processor = PDFProcessor()\n        # Add your test PDF here\n        \n        # Test analysis\n        print(\"Testing analysis...\")\n        pipeline = create_quick_pipeline()\n        # Add your test case here\n        \n        print(\"✓ All tests passed\")\n        \n    except Exception as e:\n        print(f\"✗ Error: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()\nEOF\n\nuv run python minimal_test.py\n\n\n\n\n\n\nError Message\nLikely Cause\nQuick Fix\n\n\n\n\nuv: command not found\nuv not installed\nInstall uv\n\n\nOpenAI API key is required\nMissing API key\nSet OPENAI_API_KEY\n\n\nPDF file not found\nWrong file path\nCheck file path\n\n\nNo text blocks extracted\nScanned PDF\nUse OCR first\n\n\nConnection timeout\nNetwork issue\nCheck connectivity\n\n\nRate limit exceeded\nToo many requests\nReduce concurrency\n\n\nModel not available\nWrong model name\nUse gpt-3.5-turbo\n\n\nJSON parsing error\nMalformed LLM response\nReduce temperature\n\n\nPermission denied\nFile permissions\nCheck file access\n\n\nOut of memory\nLarge document\nReduce block size\n\n\n\n\n\n\nCreate an issue on the project repository with:\n\nError message and full traceback\nSystem information from debug script\nMinimal reproduction case\nSteps taken to resolve the issue\nExpected vs. actual behavior\n\n\nIf none of these solutions work, please create an issue with detailed information about your environment and the specific error you’re encountering."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "VeritaScribe Documentation",
    "section": "",
    "text": "VeritaScribe is an intelligent document analysis system that automatically reviews PDF thesis documents for quality issues including grammar errors, content plausibility problems, and citation format inconsistencies.\n\n\nVeritaScribe combines advanced AI language models with structured document processing to provide comprehensive academic document review. Built with modern Python tools including DSPy for LLM orchestration, Pydantic for structured data modeling, and PyMuPDF for PDF processing.\n\n\n\n\n\n\n\nGrammar and linguistic error detection\nContent plausibility validation\n\nCitation format verification\nError severity classification\n\n\n\n\n\nDetailed error reports with locations\nVisual analytics and charts\nJSON data export\nMarkdown reports\n\n\n\n\n\nMultiple LLM model support\nCustomizable analysis parameters\nCitation style configuration\nProcessing optimization settings\n\n\n\n\n\nCommand-line interface\nQuick analysis mode\nDemo mode for testing\nComprehensive error messages\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    A[PDF Input] --&gt; B[Text Extraction]\n    B --&gt; C[LLM Analysis]\n    C --&gt; D[Error Detection]\n    D --&gt; E[Report Generation]\n    E --&gt; F[Visualizations]\n\n\n\n\n\n\n\nPDF Processing: Extracts text while preserving layout and location information\nAI Analysis: Uses large language models to analyze content for various types of errors\nError Classification: Categorizes and scores errors by type and severity\nReport Generation: Creates comprehensive reports and visualizations\n\n\n\n\nGet started with VeritaScribe in just a few steps:\n\nInstall dependencies:\nuv sync\nConfigure API key:\ncp .env.example .env\n# Edit .env to add your OpenAI API key\nTry the demo:\nuv run python -m veritascribe demo\nAnalyze your document:\nuv run python -m veritascribe analyze your_thesis.pdf\n\n\n\n\n\n\n\nSpelling mistakes and typos\nGrammatical inconsistencies\nPunctuation errors\nStyle and readability issues\n\n\n\n\n\nLogical inconsistencies\nFactual accuracy concerns\nArgument structure problems\nCitation-content mismatches\n\n\n\n\n\nIncorrect citation style formatting\nMissing or incomplete references\nInconsistent bibliography formatting\nCitation accuracy issues\n\n\n\n\n\nVeritaScribe follows a modular pipeline architecture:\n\nConfiguration Layer: Environment-based settings management\nPDF Processing: Text extraction with layout preservation\nLLM Analysis: DSPy-based structured analysis modules\nData Models: Pydantic schemas for type safety\nReport Generation: Multi-format output with visualizations\n\n\n\n\n\nInstallation Guide: Detailed setup instructions\nConfiguration Reference: Complete configuration options\nUsage Guide: Comprehensive usage examples\nAPI Reference: Technical documentation\nArchitecture Guide: System design and development\n\n\n\n\nIf you encounter issues or have questions:\n\nCheck the Troubleshooting Guide\nRun system diagnostics: uv run python -m veritascribe test\nReview configuration: uv run python -m veritascribe config\n\n\nVeritaScribe is designed for defensive security and academic quality assurance purposes only.",
    "crumbs": [
      "Home",
      "VeritaScribe Documentation"
    ]
  },
  {
    "objectID": "index.html#what-is-veritascribe",
    "href": "index.html#what-is-veritascribe",
    "title": "VeritaScribe Documentation",
    "section": "",
    "text": "VeritaScribe combines advanced AI language models with structured document processing to provide comprehensive academic document review. Built with modern Python tools including DSPy for LLM orchestration, Pydantic for structured data modeling, and PyMuPDF for PDF processing.",
    "crumbs": [
      "Home",
      "VeritaScribe Documentation"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "VeritaScribe Documentation",
    "section": "",
    "text": "Grammar and linguistic error detection\nContent plausibility validation\n\nCitation format verification\nError severity classification\n\n\n\n\n\nDetailed error reports with locations\nVisual analytics and charts\nJSON data export\nMarkdown reports\n\n\n\n\n\nMultiple LLM model support\nCustomizable analysis parameters\nCitation style configuration\nProcessing optimization settings\n\n\n\n\n\nCommand-line interface\nQuick analysis mode\nDemo mode for testing\nComprehensive error messages",
    "crumbs": [
      "Home",
      "VeritaScribe Documentation"
    ]
  },
  {
    "objectID": "index.html#how-it-works",
    "href": "index.html#how-it-works",
    "title": "VeritaScribe Documentation",
    "section": "",
    "text": "flowchart LR\n    A[PDF Input] --&gt; B[Text Extraction]\n    B --&gt; C[LLM Analysis]\n    C --&gt; D[Error Detection]\n    D --&gt; E[Report Generation]\n    E --&gt; F[Visualizations]\n\n\n\n\n\n\n\nPDF Processing: Extracts text while preserving layout and location information\nAI Analysis: Uses large language models to analyze content for various types of errors\nError Classification: Categorizes and scores errors by type and severity\nReport Generation: Creates comprehensive reports and visualizations",
    "crumbs": [
      "Home",
      "VeritaScribe Documentation"
    ]
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "VeritaScribe Documentation",
    "section": "",
    "text": "Get started with VeritaScribe in just a few steps:\n\nInstall dependencies:\nuv sync\nConfigure API key:\ncp .env.example .env\n# Edit .env to add your OpenAI API key\nTry the demo:\nuv run python -m veritascribe demo\nAnalyze your document:\nuv run python -m veritascribe analyze your_thesis.pdf",
    "crumbs": [
      "Home",
      "VeritaScribe Documentation"
    ]
  },
  {
    "objectID": "index.html#error-types-detected",
    "href": "index.html#error-types-detected",
    "title": "VeritaScribe Documentation",
    "section": "",
    "text": "Spelling mistakes and typos\nGrammatical inconsistencies\nPunctuation errors\nStyle and readability issues\n\n\n\n\n\nLogical inconsistencies\nFactual accuracy concerns\nArgument structure problems\nCitation-content mismatches\n\n\n\n\n\nIncorrect citation style formatting\nMissing or incomplete references\nInconsistent bibliography formatting\nCitation accuracy issues",
    "crumbs": [
      "Home",
      "VeritaScribe Documentation"
    ]
  },
  {
    "objectID": "index.html#architecture-overview",
    "href": "index.html#architecture-overview",
    "title": "VeritaScribe Documentation",
    "section": "",
    "text": "VeritaScribe follows a modular pipeline architecture:\n\nConfiguration Layer: Environment-based settings management\nPDF Processing: Text extraction with layout preservation\nLLM Analysis: DSPy-based structured analysis modules\nData Models: Pydantic schemas for type safety\nReport Generation: Multi-format output with visualizations",
    "crumbs": [
      "Home",
      "VeritaScribe Documentation"
    ]
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "VeritaScribe Documentation",
    "section": "",
    "text": "Installation Guide: Detailed setup instructions\nConfiguration Reference: Complete configuration options\nUsage Guide: Comprehensive usage examples\nAPI Reference: Technical documentation\nArchitecture Guide: System design and development",
    "crumbs": [
      "Home",
      "VeritaScribe Documentation"
    ]
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "VeritaScribe Documentation",
    "section": "",
    "text": "If you encounter issues or have questions:\n\nCheck the Troubleshooting Guide\nRun system diagnostics: uv run python -m veritascribe test\nReview configuration: uv run python -m veritascribe config\n\n\nVeritaScribe is designed for defensive security and academic quality assurance purposes only.",
    "crumbs": [
      "Home",
      "VeritaScribe Documentation"
    ]
  },
  {
    "objectID": "configuration.html",
    "href": "configuration.html",
    "title": "Configuration Reference",
    "section": "",
    "text": "VeritaScribe uses environment variables for configuration, providing flexibility and security for different deployment scenarios.\n\n\nConfiguration is managed through:\n\nEnvironment variables (highest priority)\n.env file (for local development)\nDefault values (built into the application)\n\n\n\n\n\n\nThe only required configuration parameter:\nOPENAI_API_KEY=your_openai_api_key_here\n\n\n\n\n\n\nAPI Key Security\n\n\n\n\nNever commit your API key to version control\nUse environment variables in production\nThe .env file is already in .gitignore\nRotate keys regularly for security\n\n\n\n\n\n\n\nVisit OpenAI Platform\nSign in or create an account\nClick “Create new secret key”\nCopy the key and add it to your .env file\n\n\n\n\n\n\n\n# Default model for analysis (default: gpt-4)\nDEFAULT_MODEL=gpt-4\n\n# Alternative models you can use:\n# DEFAULT_MODEL=gpt-4-turbo\n# DEFAULT_MODEL=gpt-3.5-turbo\nModel Recommendations:\n\ngpt-4: Best quality, higher cost, slower\ngpt-4-turbo: Good balance of quality and speed\ngpt-3.5-turbo: Fastest, lowest cost, adequate quality\n\n\n\n\n# Maximum tokens per LLM request (default: 2000)\nMAX_TOKENS=2000\n\n# LLM temperature for consistency (default: 0.1)\n# Range: 0.0 (deterministic) to 1.0 (creative)\nTEMPERATURE=0.1\n\n\n\n\n\n\nOptimizing Token Usage\n\n\n\n\nLower MAX_TOKENS reduces costs but may truncate analysis\nHigher values allow more detailed analysis\nMonitor usage with uv run python -m veritascribe config\n\n\n\n\n\n\n\n\n\n# Grammar and linguistic analysis (default: true)\nGRAMMAR_ANALYSIS_ENABLED=true\n\n# Content plausibility checking (default: true)\nCONTENT_ANALYSIS_ENABLED=true\n\n# Citation format validation (default: true)\nCITATION_ANALYSIS_ENABLED=true\nUse Cases: - Disable expensive analysis types for cost optimization - Focus on specific error types during review phases - Customize analysis for different document types\n\n\n\n# Threshold for high severity classification (default: 0.8)\nHIGH_SEVERITY_THRESHOLD=0.8\n\n# Threshold for medium severity classification (default: 0.5)\nMEDIUM_SEVERITY_THRESHOLD=0.5\nErrors are classified as: - High: Score ≥ HIGH_SEVERITY_THRESHOLD - Medium: Score ≥ MEDIUM_SEVERITY_THRESHOLD\n- Low: Score &lt; MEDIUM_SEVERITY_THRESHOLD\n\n\n\n\n\n\n# Maximum characters per analysis block (default: 2000)\nMAX_TEXT_BLOCK_SIZE=2000\n\n# Minimum characters for analysis (default: 50)\nMIN_TEXT_BLOCK_SIZE=50\nOptimization Guidelines:\n\n\n\nDocument Size\nRecommended MAX_TEXT_BLOCK_SIZE\n\n\n\n\nSmall (&lt; 50 pages)\n2000-3000\n\n\nMedium (50-100 pages)\n1500-2000\n\n\nLarge (&gt; 100 pages)\n1000-1500\n\n\n\n\n\n\n# Enable parallel LLM requests (default: true)\nPARALLEL_PROCESSING=true\n\n# Maximum concurrent requests (default: 5)\nMAX_CONCURRENT_REQUESTS=5\n\n\n\n\n\n\nRate Limiting\n\n\n\nSetting MAX_CONCURRENT_REQUESTS too high may trigger API rate limits. Start with 3-5 and increase gradually based on your API tier.\n\n\n\n\n\n# Maximum retry attempts for failed requests (default: 3)\nMAX_RETRIES=3\n\n# Delay between retries in seconds (default: 1.0)\nRETRY_DELAY=1.0\n\n\n\n\n\n\n# Default output directory (default: ./analysis_output)\nOUTPUT_DIRECTORY=./analysis_output\n\n# Generate error visualization charts (default: true)\nGENERATE_VISUALIZATIONS=true\n\n# Save detailed text reports (default: true)\nSAVE_DETAILED_REPORTS=true\n\n\n\n\n\n\nCreate a .env.dev file for development settings:\n# Development-specific settings\nDEFAULT_MODEL=gpt-3.5-turbo\nMAX_TOKENS=1500\nPARALLEL_PROCESSING=false\nMAX_CONCURRENT_REQUESTS=2\nGENERATE_VISUALIZATIONS=true\n\n\n\nSet environment variables directly:\nexport OPENAI_API_KEY=\"your-production-key\"\nexport DEFAULT_MODEL=\"gpt-4\"\nexport MAX_TOKENS=2000\nexport PARALLEL_PROCESSING=true\nexport MAX_CONCURRENT_REQUESTS=10\nexport OUTPUT_DIRECTORY=\"/app/analysis_output\"\n\n\n\nExample docker-compose.yml:\nversion: '3.8'\nservices:\n  veritascribe:\n    build: .\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - DEFAULT_MODEL=gpt-4-turbo\n      - MAX_TOKENS=2000\n      - PARALLEL_PROCESSING=true\n      - MAX_CONCURRENT_REQUESTS=5\n    volumes:\n      - ./analysis_output:/app/analysis_output\n\n\n\n\n\n\nCheck your current settings:\nuv run python -m veritascribe config\nOutput example:\nVeritaScribe Configuration\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Setting           ┃ Value                                                                                                    ┃ Description                                                                                                              ┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ Default Model     │ gpt-4                                                                                                    │ LLM model for analysis                                                                                                  │\n│ Max Tokens        │ 2000                                                                                                     │ Maximum tokens per request                                                                                              │\n│ Temperature       │ 0.1                                                                                                      │ LLM temperature setting                                                                                                 │\n│ Grammar Analysis  │ ✓                                                                                                        │ Grammar checking enabled                                                                                                │\n│ Content Analysis  │ ✓                                                                                                        │ Content validation enabled                                                                                              │\n│ Citation Analysis │ ✓                                                                                                        │ Citation checking enabled                                                                                               │\n│ Parallel Process… │ ✓                                                                                                        │ Parallel LLM requests                                                                                                   │\n│ Max Concurrent    │ 5                                                                                                        │ Maximum parallel requests                                                                                               │\n│ Output Directory  │ ./analysis_output                                                                                        │ Default output location                                                                                                  │\n│ Max Retries       │ 3                                                                                                        │ LLM request retry limit                                                                                                 │\n└───────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n✓ OpenAI API key is configured\n\n\n\nValidate your configuration with system tests:\nuv run python -m veritascribe test\n\n\n\n\n\n\nFor budget-conscious usage:\nDEFAULT_MODEL=gpt-3.5-turbo\nMAX_TOKENS=1500\nMAX_TEXT_BLOCK_SIZE=1500\nPARALLEL_PROCESSING=false\nMAX_CONCURRENT_REQUESTS=2\n\n\n\nFor faster processing:\nDEFAULT_MODEL=gpt-4-turbo\nPARALLEL_PROCESSING=true\nMAX_CONCURRENT_REQUESTS=10\nMAX_RETRIES=1\nRETRY_DELAY=0.5\n\n\n\nFor highest quality analysis:\nDEFAULT_MODEL=gpt-4\nMAX_TOKENS=3000\nTEMPERATURE=0.0\nMAX_TEXT_BLOCK_SIZE=2500\nPARALLEL_PROCESSING=true\nMAX_CONCURRENT_REQUESTS=3\n\n\n\n\n\n\nAPI Key Not Working\n# Test API key validity\nuv run python -c \"\nfrom openai import OpenAI\nclient = OpenAI()\nprint('API key is valid')\n\"\nModel Not Available - Check OpenAI model availability - Verify your API tier supports the model - Try with gpt-3.5-turbo as fallback\nPermission Errors - Ensure output directory is writable - Check file permissions on .env file\n\n\n\nConfiguration is loaded in this order (highest to lowest priority):\n\nEnvironment variables (e.g., export OPENAI_API_KEY=...)\n.env file in project root\nDefault values in code\n\n\n\n\n# Show all environment variables\nuv run python -c \"\nimport os\nfrom veritascribe.config import load_settings\nsettings = load_settings()\nprint('Configuration loaded successfully')\nprint(f'Model: {settings.default_model}')\nprint(f'API Key configured: {bool(settings.openai_api_key)}')\n\"\n\n\n\n\n\n\nYou can create specialized configurations for different types of analysis:\nGrammar-Only Analysis:\nGRAMMAR_ANALYSIS_ENABLED=true\nCONTENT_ANALYSIS_ENABLED=false\nCITATION_ANALYSIS_ENABLED=false\nCitation-Only Analysis:\nGRAMMAR_ANALYSIS_ENABLED=false\nCONTENT_ANALYSIS_ENABLED=false\nCITATION_ANALYSIS_ENABLED=true\n\n\n\nWhile VeritaScribe currently supports one model at a time, you can create different configurations for different use cases by using multiple .env files:\n# Load specific configuration\nENV_FILE=.env.production uv run python -m veritascribe analyze thesis.pdf\n\nFor more advanced configuration scenarios, see the Architecture Guide.",
    "crumbs": [
      "Home",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "configuration.html#configuration-overview",
    "href": "configuration.html#configuration-overview",
    "title": "Configuration Reference",
    "section": "",
    "text": "Configuration is managed through:\n\nEnvironment variables (highest priority)\n.env file (for local development)\nDefault values (built into the application)",
    "crumbs": [
      "Home",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "configuration.html#required-configuration",
    "href": "configuration.html#required-configuration",
    "title": "Configuration Reference",
    "section": "",
    "text": "The only required configuration parameter:\nOPENAI_API_KEY=your_openai_api_key_here\n\n\n\n\n\n\nAPI Key Security\n\n\n\n\nNever commit your API key to version control\nUse environment variables in production\nThe .env file is already in .gitignore\nRotate keys regularly for security\n\n\n\n\n\n\n\nVisit OpenAI Platform\nSign in or create an account\nClick “Create new secret key”\nCopy the key and add it to your .env file",
    "crumbs": [
      "Home",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "configuration.html#llm-model-configuration",
    "href": "configuration.html#llm-model-configuration",
    "title": "Configuration Reference",
    "section": "",
    "text": "# Default model for analysis (default: gpt-4)\nDEFAULT_MODEL=gpt-4\n\n# Alternative models you can use:\n# DEFAULT_MODEL=gpt-4-turbo\n# DEFAULT_MODEL=gpt-3.5-turbo\nModel Recommendations:\n\ngpt-4: Best quality, higher cost, slower\ngpt-4-turbo: Good balance of quality and speed\ngpt-3.5-turbo: Fastest, lowest cost, adequate quality\n\n\n\n\n# Maximum tokens per LLM request (default: 2000)\nMAX_TOKENS=2000\n\n# LLM temperature for consistency (default: 0.1)\n# Range: 0.0 (deterministic) to 1.0 (creative)\nTEMPERATURE=0.1\n\n\n\n\n\n\nOptimizing Token Usage\n\n\n\n\nLower MAX_TOKENS reduces costs but may truncate analysis\nHigher values allow more detailed analysis\nMonitor usage with uv run python -m veritascribe config",
    "crumbs": [
      "Home",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "configuration.html#analysis-feature-configuration",
    "href": "configuration.html#analysis-feature-configuration",
    "title": "Configuration Reference",
    "section": "",
    "text": "# Grammar and linguistic analysis (default: true)\nGRAMMAR_ANALYSIS_ENABLED=true\n\n# Content plausibility checking (default: true)\nCONTENT_ANALYSIS_ENABLED=true\n\n# Citation format validation (default: true)\nCITATION_ANALYSIS_ENABLED=true\nUse Cases: - Disable expensive analysis types for cost optimization - Focus on specific error types during review phases - Customize analysis for different document types\n\n\n\n# Threshold for high severity classification (default: 0.8)\nHIGH_SEVERITY_THRESHOLD=0.8\n\n# Threshold for medium severity classification (default: 0.5)\nMEDIUM_SEVERITY_THRESHOLD=0.5\nErrors are classified as: - High: Score ≥ HIGH_SEVERITY_THRESHOLD - Medium: Score ≥ MEDIUM_SEVERITY_THRESHOLD\n- Low: Score &lt; MEDIUM_SEVERITY_THRESHOLD",
    "crumbs": [
      "Home",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "configuration.html#processing-configuration",
    "href": "configuration.html#processing-configuration",
    "title": "Configuration Reference",
    "section": "",
    "text": "# Maximum characters per analysis block (default: 2000)\nMAX_TEXT_BLOCK_SIZE=2000\n\n# Minimum characters for analysis (default: 50)\nMIN_TEXT_BLOCK_SIZE=50\nOptimization Guidelines:\n\n\n\nDocument Size\nRecommended MAX_TEXT_BLOCK_SIZE\n\n\n\n\nSmall (&lt; 50 pages)\n2000-3000\n\n\nMedium (50-100 pages)\n1500-2000\n\n\nLarge (&gt; 100 pages)\n1000-1500\n\n\n\n\n\n\n# Enable parallel LLM requests (default: true)\nPARALLEL_PROCESSING=true\n\n# Maximum concurrent requests (default: 5)\nMAX_CONCURRENT_REQUESTS=5\n\n\n\n\n\n\nRate Limiting\n\n\n\nSetting MAX_CONCURRENT_REQUESTS too high may trigger API rate limits. Start with 3-5 and increase gradually based on your API tier.\n\n\n\n\n\n# Maximum retry attempts for failed requests (default: 3)\nMAX_RETRIES=3\n\n# Delay between retries in seconds (default: 1.0)\nRETRY_DELAY=1.0",
    "crumbs": [
      "Home",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "configuration.html#output-configuration",
    "href": "configuration.html#output-configuration",
    "title": "Configuration Reference",
    "section": "",
    "text": "# Default output directory (default: ./analysis_output)\nOUTPUT_DIRECTORY=./analysis_output\n\n# Generate error visualization charts (default: true)\nGENERATE_VISUALIZATIONS=true\n\n# Save detailed text reports (default: true)\nSAVE_DETAILED_REPORTS=true",
    "crumbs": [
      "Home",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "configuration.html#environment-specific-configuration",
    "href": "configuration.html#environment-specific-configuration",
    "title": "Configuration Reference",
    "section": "",
    "text": "Create a .env.dev file for development settings:\n# Development-specific settings\nDEFAULT_MODEL=gpt-3.5-turbo\nMAX_TOKENS=1500\nPARALLEL_PROCESSING=false\nMAX_CONCURRENT_REQUESTS=2\nGENERATE_VISUALIZATIONS=true\n\n\n\nSet environment variables directly:\nexport OPENAI_API_KEY=\"your-production-key\"\nexport DEFAULT_MODEL=\"gpt-4\"\nexport MAX_TOKENS=2000\nexport PARALLEL_PROCESSING=true\nexport MAX_CONCURRENT_REQUESTS=10\nexport OUTPUT_DIRECTORY=\"/app/analysis_output\"\n\n\n\nExample docker-compose.yml:\nversion: '3.8'\nservices:\n  veritascribe:\n    build: .\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - DEFAULT_MODEL=gpt-4-turbo\n      - MAX_TOKENS=2000\n      - PARALLEL_PROCESSING=true\n      - MAX_CONCURRENT_REQUESTS=5\n    volumes:\n      - ./analysis_output:/app/analysis_output",
    "crumbs": [
      "Home",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "configuration.html#configuration-validation",
    "href": "configuration.html#configuration-validation",
    "title": "Configuration Reference",
    "section": "",
    "text": "Check your current settings:\nuv run python -m veritascribe config\nOutput example:\nVeritaScribe Configuration\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Setting           ┃ Value                                                                                                    ┃ Description                                                                                                              ┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ Default Model     │ gpt-4                                                                                                    │ LLM model for analysis                                                                                                  │\n│ Max Tokens        │ 2000                                                                                                     │ Maximum tokens per request                                                                                              │\n│ Temperature       │ 0.1                                                                                                      │ LLM temperature setting                                                                                                 │\n│ Grammar Analysis  │ ✓                                                                                                        │ Grammar checking enabled                                                                                                │\n│ Content Analysis  │ ✓                                                                                                        │ Content validation enabled                                                                                              │\n│ Citation Analysis │ ✓                                                                                                        │ Citation checking enabled                                                                                               │\n│ Parallel Process… │ ✓                                                                                                        │ Parallel LLM requests                                                                                                   │\n│ Max Concurrent    │ 5                                                                                                        │ Maximum parallel requests                                                                                               │\n│ Output Directory  │ ./analysis_output                                                                                        │ Default output location                                                                                                  │\n│ Max Retries       │ 3                                                                                                        │ LLM request retry limit                                                                                                 │\n└───────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n✓ OpenAI API key is configured\n\n\n\nValidate your configuration with system tests:\nuv run python -m veritascribe test",
    "crumbs": [
      "Home",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "configuration.html#performance-tuning",
    "href": "configuration.html#performance-tuning",
    "title": "Configuration Reference",
    "section": "",
    "text": "For budget-conscious usage:\nDEFAULT_MODEL=gpt-3.5-turbo\nMAX_TOKENS=1500\nMAX_TEXT_BLOCK_SIZE=1500\nPARALLEL_PROCESSING=false\nMAX_CONCURRENT_REQUESTS=2\n\n\n\nFor faster processing:\nDEFAULT_MODEL=gpt-4-turbo\nPARALLEL_PROCESSING=true\nMAX_CONCURRENT_REQUESTS=10\nMAX_RETRIES=1\nRETRY_DELAY=0.5\n\n\n\nFor highest quality analysis:\nDEFAULT_MODEL=gpt-4\nMAX_TOKENS=3000\nTEMPERATURE=0.0\nMAX_TEXT_BLOCK_SIZE=2500\nPARALLEL_PROCESSING=true\nMAX_CONCURRENT_REQUESTS=3",
    "crumbs": [
      "Home",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "configuration.html#troubleshooting-configuration",
    "href": "configuration.html#troubleshooting-configuration",
    "title": "Configuration Reference",
    "section": "",
    "text": "API Key Not Working\n# Test API key validity\nuv run python -c \"\nfrom openai import OpenAI\nclient = OpenAI()\nprint('API key is valid')\n\"\nModel Not Available - Check OpenAI model availability - Verify your API tier supports the model - Try with gpt-3.5-turbo as fallback\nPermission Errors - Ensure output directory is writable - Check file permissions on .env file\n\n\n\nConfiguration is loaded in this order (highest to lowest priority):\n\nEnvironment variables (e.g., export OPENAI_API_KEY=...)\n.env file in project root\nDefault values in code\n\n\n\n\n# Show all environment variables\nuv run python -c \"\nimport os\nfrom veritascribe.config import load_settings\nsettings = load_settings()\nprint('Configuration loaded successfully')\nprint(f'Model: {settings.default_model}')\nprint(f'API Key configured: {bool(settings.openai_api_key)}')\n\"",
    "crumbs": [
      "Home",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "configuration.html#advanced-configuration",
    "href": "configuration.html#advanced-configuration",
    "title": "Configuration Reference",
    "section": "",
    "text": "You can create specialized configurations for different types of analysis:\nGrammar-Only Analysis:\nGRAMMAR_ANALYSIS_ENABLED=true\nCONTENT_ANALYSIS_ENABLED=false\nCITATION_ANALYSIS_ENABLED=false\nCitation-Only Analysis:\nGRAMMAR_ANALYSIS_ENABLED=false\nCONTENT_ANALYSIS_ENABLED=false\nCITATION_ANALYSIS_ENABLED=true\n\n\n\nWhile VeritaScribe currently supports one model at a time, you can create different configurations for different use cases by using multiple .env files:\n# Load specific configuration\nENV_FILE=.env.production uv run python -m veritascribe analyze thesis.pdf\n\nFor more advanced configuration scenarios, see the Architecture Guide.",
    "crumbs": [
      "Home",
      "Configuration Reference"
    ]
  },
  {
    "objectID": "api-reference.html",
    "href": "api-reference.html",
    "title": "API Reference",
    "section": "",
    "text": "This reference covers VeritaScribe’s programmatic interfaces, data models, and internal APIs for developers and advanced users.\n\n\n\n\n\n\nPerform comprehensive thesis analysis.\nuv run python -m veritascribe analyze [OPTIONS] PDF_PATH\nArguments: - PDF_PATH (required): Path to the PDF thesis file\nOptions: - --output, -o TEXT: Output directory for reports - --citation-style, -c TEXT: Expected citation style (APA, MLA, Chicago, IEEE, Harvard) - --quick, -q: Perform quick analysis (first 10 blocks only) - --no-viz: Skip generating visualization charts - --verbose, -v: Enable verbose logging\nExit Codes: - 0: Success - 1: Error (file not found, analysis failed, etc.)\n\n\n\nFast analysis of document subset.\nuv run python -m veritascribe quick [OPTIONS] PDF_PATH\nArguments: - PDF_PATH (required): Path to the PDF thesis file\nOptions: - --blocks, -b INTEGER: Number of text blocks to analyze (default: 5)\n\n\n\nCreate and analyze sample document.\nuv run python -m veritascribe demo\nNo arguments or options. Creates demo_thesis.pdf in current directory.\n\n\n\nDisplay current configuration.\nuv run python -m veritascribe config\nOutputs configuration table and API key status.\n\n\n\nRun system diagnostics.\nuv run python -m veritascribe test\nTests configuration loading, PDF processing, and LLM connectivity.\n\n\n\n\n\n\n\n\n\nMain configuration class using Pydantic Settings.\nfrom veritascribe.config import VeritaScribeSettings\n\nsettings = VeritaScribeSettings(\n    openai_api_key=\"your-key-here\",\n    default_model=\"gpt-4\",\n    max_tokens=2000,\n    temperature=0.1\n)\nKey Fields:\n\n\n\n\n\n\n\n\n\nField\nType\nDefault\nDescription\n\n\n\n\nopenai_api_key\nOptional[str]\nNone\nOpenAI API key\n\n\ndefault_model\nstr\n\"gpt-4\"\nLLM model name\n\n\nmax_tokens\nint\n2000\nMax tokens per request\n\n\ntemperature\nfloat\n0.1\nLLM temperature\n\n\ngrammar_analysis_enabled\nbool\nTrue\nEnable grammar analysis\n\n\ncontent_analysis_enabled\nbool\nTrue\nEnable content analysis\n\n\ncitation_analysis_enabled\nbool\nTrue\nEnable citation analysis\n\n\nhigh_severity_threshold\nfloat\n0.8\nHigh severity threshold\n\n\nmedium_severity_threshold\nfloat\n0.5\nMedium severity threshold\n\n\nmax_text_block_size\nint\n2000\nMax chars per block\n\n\nparallel_processing\nbool\nTrue\nEnable parallel processing\n\n\nmax_concurrent_requests\nint\n5\nMax concurrent requests\n\n\noutput_directory\nstr\n\"./analysis_output\"\nDefault output dir\n\n\nmax_retries\nint\n3\nMax retry attempts\n\n\nretry_delay\nfloat\n1.0\nRetry delay seconds\n\n\n\n\n\n\nfrom veritascribe.config import (\n    load_settings,\n    get_settings,\n    initialize_system\n)\n\n# Load fresh settings\nsettings = load_settings()\n\n# Get global settings instance (cached)\nsettings = get_settings()\n\n# Initialize complete system\nsettings, dspy_config = initialize_system()\n\n\n\n\nAll data models use Pydantic for validation and serialization.\n\n\nRepresents the location of text or errors in the document.\nfrom veritascribe.data_models import LocationHint\n\nlocation = LocationHint(\n    page_number=12,\n    bounding_box_coordinates=[72, 345, 520, 365]\n)\nFields: - page_number: int - Page number (1-indexed) - bounding_box_coordinates: Optional[List[float]] - [x1, y1, x2, y2] coordinates\n\n\n\nBase class for all error types.\nfrom veritascribe.data_models import BaseError, ErrorSeverity, LocationHint\n\nerror = BaseError(\n    error_type=\"grammar\",\n    severity=ErrorSeverity.HIGH,\n    original_text=\"The results shows that...\",\n    suggested_correction=\"The results show that...\",\n    explanation=\"Subject-verb agreement error.\",\n    location=LocationHint(page_number=12),\n    confidence_score=0.95\n)\nFields: - error_type: str - Type of error (grammar, citation_format, content_plausibility) - severity: ErrorSeverity - Error severity level - original_text: str - Original problematic text - suggested_correction: Optional[str] - Suggested fix - explanation: str - Explanation of the error - location: LocationHint - Error location - confidence_score: float - Confidence in error detection (0.0-1.0)\n\n\n\nEnumeration for error severity levels.\nfrom veritascribe.data_models import ErrorSeverity\n\n# Available values\nErrorSeverity.LOW      # Low priority\nErrorSeverity.MEDIUM   # Medium priority  \nErrorSeverity.HIGH     # High priority\n\n\n\nRepresents extracted text with layout information.\nfrom veritascribe.data_models import TextBlock\n\nblock = TextBlock(\n    content=\"The methodology employed in this study...\",\n    page_number=5,\n    bounding_box_coordinates=[72, 200, 520, 280]\n)\nFields: - content: str - Extracted text content - page_number: int - Page number (1-indexed) - bounding_box_coordinates: Optional[List[float]] - Layout coordinates\n\n\n\nLinks a text block with its detected errors.\nfrom veritascribe.data_models import AnalysisResult, TextBlock, BaseError\n\nresult = AnalysisResult(\n    text_block=text_block,\n    errors=[error1, error2, error3]\n)\nFields: - text_block: TextBlock - The analyzed text block - errors: List[BaseError] - List of errors found in this block\n\n\n\nComplete analysis report containing all results.\nfrom veritascribe.data_models import ThesisAnalysisReport\n\nreport = ThesisAnalysisReport(\n    document_name=\"thesis.pdf\",\n    total_pages=45,\n    total_words=12543,\n    total_text_blocks=87,\n    analysis_results=[result1, result2, ...],\n    total_processing_time_seconds=45.32\n)\nKey Fields: - document_name: str - Name of analyzed document - analysis_timestamp: datetime - When analysis was performed - total_pages: int - Number of pages in document - total_words: int - Estimated word count - total_text_blocks: int - Number of text blocks analyzed - analysis_results: List[AnalysisResult] - All analysis results - total_processing_time_seconds: float - Processing time\nComputed Properties: - total_errors: int - Total number of errors found - error_rate: float - Errors per 1,000 words - errors_by_type: Dict[str, int] - Error counts by type - errors_by_severity: Dict[str, int] - Error counts by severity\nMethods:\n# Get errors by severity\nhigh_errors = report.get_high_severity_errors()\nmedium_errors = report.get_medium_severity_errors()\nlow_errors = report.get_low_severity_errors()\n\n# Get errors by type\ngrammar_errors = report.get_errors_by_type(\"grammar\")\ncitation_errors = report.get_errors_by_type(\"citation_format\")\n\n\n\n\n\n\nMain class for PDF text extraction.\nfrom veritascribe.pdf_processor import PDFProcessor\n\nprocessor = PDFProcessor()\ntext_blocks = processor.extract_text_blocks_from_pdf(\"thesis.pdf\")\nMethods:\ndef extract_text_blocks_from_pdf(\n    self, \n    pdf_path: str\n) -&gt; List[TextBlock]:\n    \"\"\"Extract text blocks from PDF with layout information.\"\"\"\n    pass\n\ndef estimate_word_count(\n    self, \n    text_blocks: List[TextBlock]\n) -&gt; int:\n    \"\"\"Estimate total word count from text blocks.\"\"\"\n    pass\n\n\n\nfrom veritascribe.pdf_processor import create_test_pdf\n\n# Create sample PDF for testing\ncreate_test_pdf(\"sample.pdf\")\n\n\n\n\n\n\nfrom veritascribe.llm_modules import (\n    LinguisticAnalysisSignature,\n    ContentValidationSignature,\n    CitationAnalysisSignature\n)\n\n# Example usage (typically internal)\nimport dspy\n\nlinguistic_analyzer = dspy.Predict(LinguisticAnalysisSignature)\nresult = linguistic_analyzer(text_chunk=\"The results shows...\")\n\n\n\nfrom veritascribe.llm_modules import (\n    LinguisticAnalyzer,\n    ContentValidator,\n    CitationChecker\n)\n\n# Initialize analyzers\nlinguistic = LinguisticAnalyzer()\ncontent = ContentValidator()\ncitation = CitationChecker()\n\n# Analyze text block\ntext_block = TextBlock(content=\"Sample text...\", page_number=1)\n\ngrammar_errors = linguistic.analyze(text_block)\ncontent_errors = content.analyze(text_block)\ncitation_errors = citation.analyze(text_block, bibliography=\"References...\")\n\n\n\n\n\n\nMain orchestration class for analysis workflow.\nfrom veritascribe.pipeline import AnalysisPipeline\n\npipeline = AnalysisPipeline()\nreport = pipeline.analyze_thesis(\n    pdf_path=\"thesis.pdf\",\n    output_dir=\"./results\",\n    citation_style=\"APA\"\n)\nMethods:\ndef analyze_thesis(\n    self,\n    pdf_path: str,\n    output_dir: str,\n    citation_style: str = \"APA\"\n) -&gt; ThesisAnalysisReport:\n    \"\"\"Perform complete thesis analysis.\"\"\"\n    pass\n\ndef quick_analyze(\n    self,\n    pdf_path: str,\n    max_blocks: int = 10\n) -&gt; ThesisAnalysisReport:\n    \"\"\"Perform quick analysis on subset of document.\"\"\"\n    pass\n\n\n\nfrom veritascribe.pipeline import (\n    create_analysis_pipeline,\n    create_quick_pipeline\n)\n\n# Create configured pipeline instances\nfull_pipeline = create_analysis_pipeline()\nquick_pipeline = create_quick_pipeline()\n\n\n\n\n\n\nHandles report generation and visualization.\nfrom veritascribe.report_generator import ReportGenerator\n\ngenerator = ReportGenerator()\n\n# Generate text report\ngenerator.generate_text_report(report, \"report.md\")\n\n# Export JSON data\ngenerator.export_json_report(report, \"data.json\")\n\n# Create visualizations\nviz_files = generator.visualize_errors(report, \"./viz_output\")\n\n# Create summary\nsummary = generator.create_summary_report(report)\nMethods:\ndef generate_text_report(\n    self,\n    report: ThesisAnalysisReport,\n    output_path: str\n) -&gt; None:\n    \"\"\"Generate comprehensive Markdown report.\"\"\"\n    pass\n\ndef export_json_report(\n    self,\n    report: ThesisAnalysisReport,\n    output_path: str\n) -&gt; None:\n    \"\"\"Export structured JSON data.\"\"\"\n    pass\n\ndef visualize_errors(\n    self,\n    report: ThesisAnalysisReport,\n    output_dir: str\n) -&gt; List[str]:\n    \"\"\"Generate error visualization charts.\"\"\"\n    pass\n\ndef create_summary_report(\n    self,\n    report: ThesisAnalysisReport\n) -&gt; Dict[str, Any]:\n    \"\"\"Create executive summary.\"\"\"\n    pass\n\n\n\n\n\n\n\nVeritaScribe defines custom exceptions for different error conditions:\nfrom veritascribe.exceptions import (\n    VeritaScribeError,\n    ConfigurationError,\n    PDFProcessingError,\n    LLMAnalysisError,\n    ReportGenerationError\n)\n\ntry:\n    settings = load_settings()\nexcept ConfigurationError as e:\n    print(f\"Configuration error: {e}\")\n\ntry:\n    blocks = processor.extract_text_blocks_from_pdf(\"invalid.pdf\")\nexcept PDFProcessingError as e:\n    print(f\"PDF processing failed: {e}\")\n\n\n\nErrors include contextual information for debugging:\ntry:\n    pipeline.analyze_thesis(\"thesis.pdf\", \"./output\")\nexcept LLMAnalysisError as e:\n    print(f\"Analysis failed: {e}\")\n    print(f\"Block being processed: {e.context.get('text_block')}\")\n    print(f\"Error type: {e.context.get('analysis_type')}\")\n\n\n\n\n\n\nExtend the error system:\nfrom veritascribe.data_models import BaseError, ErrorSeverity, LocationHint\n\nclass StyleError(BaseError):\n    \"\"\"Custom error type for style issues.\"\"\"\n    \n    def __init__(self, **kwargs):\n        super().__init__(\n            error_type=\"style\",\n            **kwargs\n        )\n\n# Usage\nstyle_error = StyleError(\n    severity=ErrorSeverity.LOW,\n    original_text=\"utilise\",\n    suggested_correction=\"utilize\", \n    explanation=\"Prefer American spelling\",\n    location=LocationHint(page_number=10)\n)\n\n\n\nCreate custom DSPy analysis modules:\nimport dspy\nfrom veritascribe.data_models import BaseError, TextBlock\n\nclass CustomAnalysisSignature(dspy.Signature):\n    \"\"\"Custom analysis signature.\"\"\"\n    text_chunk = dspy.InputField(desc=\"Text to analyze\")\n    custom_errors = dspy.OutputField(desc=\"JSON list of custom errors\")\n\nclass CustomAnalyzer:\n    \"\"\"Custom analysis module.\"\"\"\n    \n    def __init__(self):\n        self.analyzer = dspy.Predict(CustomAnalysisSignature)\n    \n    def analyze(self, text_block: TextBlock) -&gt; List[BaseError]:\n        result = self.analyzer(text_chunk=text_block.content)\n        # Parse result.custom_errors and return BaseError objects\n        return self._parse_errors(result.custom_errors, text_block)\n\n\n\nAdd custom configuration options:\nfrom veritascribe.config import VeritaScribeSettings\nfrom pydantic import Field\n\nclass ExtendedSettings(VeritaScribeSettings):\n    \"\"\"Extended configuration with custom options.\"\"\"\n    \n    custom_analysis_enabled: bool = Field(\n        default=False,\n        description=\"Enable custom analysis module\"\n    )\n    \n    custom_threshold: float = Field(\n        default=0.7,\n        description=\"Custom analysis threshold\"\n    )\n\n\n\n\n\n\nfrom veritascribe.testing import (\n    create_test_document,\n    create_test_errors,\n    mock_llm_response\n)\n\n# Create test documents\ntest_pdf = create_test_document(\n    content=\"Sample thesis content...\",\n    filename=\"test.pdf\"\n)\n\n# Create test errors\ntest_errors = create_test_errors(\n    count=5,\n    error_types=[\"grammar\", \"citation_format\"]\n)\n\n# Mock LLM responses for testing\nwith mock_llm_response(test_errors):\n    result = analyzer.analyze(text_block)\n\n\n\nimport pytest\nfrom veritascribe.testing import AnalysisFixture\n\n@pytest.fixture\ndef analysis_fixture():\n    return AnalysisFixture(\n        document_pages=10,\n        error_count=5,\n        include_visualizations=True\n    )\n\ndef test_analysis_pipeline(analysis_fixture):\n    report = analysis_fixture.run_analysis()\n    assert report.total_errors == 5\n\n\n\n\n\n\n\nBatch Processing: Process multiple blocks efficiently\nCaching: Cache LLM responses when possible\nAsync Processing: Use parallel processing for large documents\nMemory Management: Handle large PDFs without memory issues\n\n\n\n\nfrom veritascribe.monitoring import (\n    track_token_usage,\n    measure_processing_time,\n    log_api_calls\n)\n\n@track_token_usage\n@measure_processing_time\ndef analyze_with_monitoring(text_block):\n    return analyzer.analyze(text_block)\n\n\n\n\n\n\n\nNever log API keys\nUse environment variables in production\nRotate keys regularly\nMonitor usage for anomalies\n\n\n\n\nAll inputs are validated using Pydantic models:\nfrom pydantic import ValidationError\n\ntry:\n    settings = VeritaScribeSettings(temperature=2.0)  # Invalid\nexcept ValidationError as e:\n    print(\"Invalid temperature value\")\n\n\n\nReports are sanitized to prevent injection attacks:\nfrom veritascribe.security import sanitize_output\n\nsafe_content = sanitize_output(user_content)\n\nThis API reference covers the main interfaces. For implementation details, see the Architecture Guide."
  },
  {
    "objectID": "api-reference.html#command-line-interface",
    "href": "api-reference.html#command-line-interface",
    "title": "API Reference",
    "section": "",
    "text": "Perform comprehensive thesis analysis.\nuv run python -m veritascribe analyze [OPTIONS] PDF_PATH\nArguments: - PDF_PATH (required): Path to the PDF thesis file\nOptions: - --output, -o TEXT: Output directory for reports - --citation-style, -c TEXT: Expected citation style (APA, MLA, Chicago, IEEE, Harvard) - --quick, -q: Perform quick analysis (first 10 blocks only) - --no-viz: Skip generating visualization charts - --verbose, -v: Enable verbose logging\nExit Codes: - 0: Success - 1: Error (file not found, analysis failed, etc.)\n\n\n\nFast analysis of document subset.\nuv run python -m veritascribe quick [OPTIONS] PDF_PATH\nArguments: - PDF_PATH (required): Path to the PDF thesis file\nOptions: - --blocks, -b INTEGER: Number of text blocks to analyze (default: 5)\n\n\n\nCreate and analyze sample document.\nuv run python -m veritascribe demo\nNo arguments or options. Creates demo_thesis.pdf in current directory.\n\n\n\nDisplay current configuration.\nuv run python -m veritascribe config\nOutputs configuration table and API key status.\n\n\n\nRun system diagnostics.\nuv run python -m veritascribe test\nTests configuration loading, PDF processing, and LLM connectivity."
  },
  {
    "objectID": "api-reference.html#python-api",
    "href": "api-reference.html#python-api",
    "title": "API Reference",
    "section": "",
    "text": "Main configuration class using Pydantic Settings.\nfrom veritascribe.config import VeritaScribeSettings\n\nsettings = VeritaScribeSettings(\n    openai_api_key=\"your-key-here\",\n    default_model=\"gpt-4\",\n    max_tokens=2000,\n    temperature=0.1\n)\nKey Fields:\n\n\n\n\n\n\n\n\n\nField\nType\nDefault\nDescription\n\n\n\n\nopenai_api_key\nOptional[str]\nNone\nOpenAI API key\n\n\ndefault_model\nstr\n\"gpt-4\"\nLLM model name\n\n\nmax_tokens\nint\n2000\nMax tokens per request\n\n\ntemperature\nfloat\n0.1\nLLM temperature\n\n\ngrammar_analysis_enabled\nbool\nTrue\nEnable grammar analysis\n\n\ncontent_analysis_enabled\nbool\nTrue\nEnable content analysis\n\n\ncitation_analysis_enabled\nbool\nTrue\nEnable citation analysis\n\n\nhigh_severity_threshold\nfloat\n0.8\nHigh severity threshold\n\n\nmedium_severity_threshold\nfloat\n0.5\nMedium severity threshold\n\n\nmax_text_block_size\nint\n2000\nMax chars per block\n\n\nparallel_processing\nbool\nTrue\nEnable parallel processing\n\n\nmax_concurrent_requests\nint\n5\nMax concurrent requests\n\n\noutput_directory\nstr\n\"./analysis_output\"\nDefault output dir\n\n\nmax_retries\nint\n3\nMax retry attempts\n\n\nretry_delay\nfloat\n1.0\nRetry delay seconds\n\n\n\n\n\n\nfrom veritascribe.config import (\n    load_settings,\n    get_settings,\n    initialize_system\n)\n\n# Load fresh settings\nsettings = load_settings()\n\n# Get global settings instance (cached)\nsettings = get_settings()\n\n# Initialize complete system\nsettings, dspy_config = initialize_system()\n\n\n\n\nAll data models use Pydantic for validation and serialization.\n\n\nRepresents the location of text or errors in the document.\nfrom veritascribe.data_models import LocationHint\n\nlocation = LocationHint(\n    page_number=12,\n    bounding_box_coordinates=[72, 345, 520, 365]\n)\nFields: - page_number: int - Page number (1-indexed) - bounding_box_coordinates: Optional[List[float]] - [x1, y1, x2, y2] coordinates\n\n\n\nBase class for all error types.\nfrom veritascribe.data_models import BaseError, ErrorSeverity, LocationHint\n\nerror = BaseError(\n    error_type=\"grammar\",\n    severity=ErrorSeverity.HIGH,\n    original_text=\"The results shows that...\",\n    suggested_correction=\"The results show that...\",\n    explanation=\"Subject-verb agreement error.\",\n    location=LocationHint(page_number=12),\n    confidence_score=0.95\n)\nFields: - error_type: str - Type of error (grammar, citation_format, content_plausibility) - severity: ErrorSeverity - Error severity level - original_text: str - Original problematic text - suggested_correction: Optional[str] - Suggested fix - explanation: str - Explanation of the error - location: LocationHint - Error location - confidence_score: float - Confidence in error detection (0.0-1.0)\n\n\n\nEnumeration for error severity levels.\nfrom veritascribe.data_models import ErrorSeverity\n\n# Available values\nErrorSeverity.LOW      # Low priority\nErrorSeverity.MEDIUM   # Medium priority  \nErrorSeverity.HIGH     # High priority\n\n\n\nRepresents extracted text with layout information.\nfrom veritascribe.data_models import TextBlock\n\nblock = TextBlock(\n    content=\"The methodology employed in this study...\",\n    page_number=5,\n    bounding_box_coordinates=[72, 200, 520, 280]\n)\nFields: - content: str - Extracted text content - page_number: int - Page number (1-indexed) - bounding_box_coordinates: Optional[List[float]] - Layout coordinates\n\n\n\nLinks a text block with its detected errors.\nfrom veritascribe.data_models import AnalysisResult, TextBlock, BaseError\n\nresult = AnalysisResult(\n    text_block=text_block,\n    errors=[error1, error2, error3]\n)\nFields: - text_block: TextBlock - The analyzed text block - errors: List[BaseError] - List of errors found in this block\n\n\n\nComplete analysis report containing all results.\nfrom veritascribe.data_models import ThesisAnalysisReport\n\nreport = ThesisAnalysisReport(\n    document_name=\"thesis.pdf\",\n    total_pages=45,\n    total_words=12543,\n    total_text_blocks=87,\n    analysis_results=[result1, result2, ...],\n    total_processing_time_seconds=45.32\n)\nKey Fields: - document_name: str - Name of analyzed document - analysis_timestamp: datetime - When analysis was performed - total_pages: int - Number of pages in document - total_words: int - Estimated word count - total_text_blocks: int - Number of text blocks analyzed - analysis_results: List[AnalysisResult] - All analysis results - total_processing_time_seconds: float - Processing time\nComputed Properties: - total_errors: int - Total number of errors found - error_rate: float - Errors per 1,000 words - errors_by_type: Dict[str, int] - Error counts by type - errors_by_severity: Dict[str, int] - Error counts by severity\nMethods:\n# Get errors by severity\nhigh_errors = report.get_high_severity_errors()\nmedium_errors = report.get_medium_severity_errors()\nlow_errors = report.get_low_severity_errors()\n\n# Get errors by type\ngrammar_errors = report.get_errors_by_type(\"grammar\")\ncitation_errors = report.get_errors_by_type(\"citation_format\")\n\n\n\n\n\n\nMain class for PDF text extraction.\nfrom veritascribe.pdf_processor import PDFProcessor\n\nprocessor = PDFProcessor()\ntext_blocks = processor.extract_text_blocks_from_pdf(\"thesis.pdf\")\nMethods:\ndef extract_text_blocks_from_pdf(\n    self, \n    pdf_path: str\n) -&gt; List[TextBlock]:\n    \"\"\"Extract text blocks from PDF with layout information.\"\"\"\n    pass\n\ndef estimate_word_count(\n    self, \n    text_blocks: List[TextBlock]\n) -&gt; int:\n    \"\"\"Estimate total word count from text blocks.\"\"\"\n    pass\n\n\n\nfrom veritascribe.pdf_processor import create_test_pdf\n\n# Create sample PDF for testing\ncreate_test_pdf(\"sample.pdf\")\n\n\n\n\n\n\nfrom veritascribe.llm_modules import (\n    LinguisticAnalysisSignature,\n    ContentValidationSignature,\n    CitationAnalysisSignature\n)\n\n# Example usage (typically internal)\nimport dspy\n\nlinguistic_analyzer = dspy.Predict(LinguisticAnalysisSignature)\nresult = linguistic_analyzer(text_chunk=\"The results shows...\")\n\n\n\nfrom veritascribe.llm_modules import (\n    LinguisticAnalyzer,\n    ContentValidator,\n    CitationChecker\n)\n\n# Initialize analyzers\nlinguistic = LinguisticAnalyzer()\ncontent = ContentValidator()\ncitation = CitationChecker()\n\n# Analyze text block\ntext_block = TextBlock(content=\"Sample text...\", page_number=1)\n\ngrammar_errors = linguistic.analyze(text_block)\ncontent_errors = content.analyze(text_block)\ncitation_errors = citation.analyze(text_block, bibliography=\"References...\")\n\n\n\n\n\n\nMain orchestration class for analysis workflow.\nfrom veritascribe.pipeline import AnalysisPipeline\n\npipeline = AnalysisPipeline()\nreport = pipeline.analyze_thesis(\n    pdf_path=\"thesis.pdf\",\n    output_dir=\"./results\",\n    citation_style=\"APA\"\n)\nMethods:\ndef analyze_thesis(\n    self,\n    pdf_path: str,\n    output_dir: str,\n    citation_style: str = \"APA\"\n) -&gt; ThesisAnalysisReport:\n    \"\"\"Perform complete thesis analysis.\"\"\"\n    pass\n\ndef quick_analyze(\n    self,\n    pdf_path: str,\n    max_blocks: int = 10\n) -&gt; ThesisAnalysisReport:\n    \"\"\"Perform quick analysis on subset of document.\"\"\"\n    pass\n\n\n\nfrom veritascribe.pipeline import (\n    create_analysis_pipeline,\n    create_quick_pipeline\n)\n\n# Create configured pipeline instances\nfull_pipeline = create_analysis_pipeline()\nquick_pipeline = create_quick_pipeline()\n\n\n\n\n\n\nHandles report generation and visualization.\nfrom veritascribe.report_generator import ReportGenerator\n\ngenerator = ReportGenerator()\n\n# Generate text report\ngenerator.generate_text_report(report, \"report.md\")\n\n# Export JSON data\ngenerator.export_json_report(report, \"data.json\")\n\n# Create visualizations\nviz_files = generator.visualize_errors(report, \"./viz_output\")\n\n# Create summary\nsummary = generator.create_summary_report(report)\nMethods:\ndef generate_text_report(\n    self,\n    report: ThesisAnalysisReport,\n    output_path: str\n) -&gt; None:\n    \"\"\"Generate comprehensive Markdown report.\"\"\"\n    pass\n\ndef export_json_report(\n    self,\n    report: ThesisAnalysisReport,\n    output_path: str\n) -&gt; None:\n    \"\"\"Export structured JSON data.\"\"\"\n    pass\n\ndef visualize_errors(\n    self,\n    report: ThesisAnalysisReport,\n    output_dir: str\n) -&gt; List[str]:\n    \"\"\"Generate error visualization charts.\"\"\"\n    pass\n\ndef create_summary_report(\n    self,\n    report: ThesisAnalysisReport\n) -&gt; Dict[str, Any]:\n    \"\"\"Create executive summary.\"\"\"\n    pass"
  },
  {
    "objectID": "api-reference.html#error-handling",
    "href": "api-reference.html#error-handling",
    "title": "API Reference",
    "section": "",
    "text": "VeritaScribe defines custom exceptions for different error conditions:\nfrom veritascribe.exceptions import (\n    VeritaScribeError,\n    ConfigurationError,\n    PDFProcessingError,\n    LLMAnalysisError,\n    ReportGenerationError\n)\n\ntry:\n    settings = load_settings()\nexcept ConfigurationError as e:\n    print(f\"Configuration error: {e}\")\n\ntry:\n    blocks = processor.extract_text_blocks_from_pdf(\"invalid.pdf\")\nexcept PDFProcessingError as e:\n    print(f\"PDF processing failed: {e}\")\n\n\n\nErrors include contextual information for debugging:\ntry:\n    pipeline.analyze_thesis(\"thesis.pdf\", \"./output\")\nexcept LLMAnalysisError as e:\n    print(f\"Analysis failed: {e}\")\n    print(f\"Block being processed: {e.context.get('text_block')}\")\n    print(f\"Error type: {e.context.get('analysis_type')}\")"
  },
  {
    "objectID": "api-reference.html#extensibility",
    "href": "api-reference.html#extensibility",
    "title": "API Reference",
    "section": "",
    "text": "Extend the error system:\nfrom veritascribe.data_models import BaseError, ErrorSeverity, LocationHint\n\nclass StyleError(BaseError):\n    \"\"\"Custom error type for style issues.\"\"\"\n    \n    def __init__(self, **kwargs):\n        super().__init__(\n            error_type=\"style\",\n            **kwargs\n        )\n\n# Usage\nstyle_error = StyleError(\n    severity=ErrorSeverity.LOW,\n    original_text=\"utilise\",\n    suggested_correction=\"utilize\", \n    explanation=\"Prefer American spelling\",\n    location=LocationHint(page_number=10)\n)\n\n\n\nCreate custom DSPy analysis modules:\nimport dspy\nfrom veritascribe.data_models import BaseError, TextBlock\n\nclass CustomAnalysisSignature(dspy.Signature):\n    \"\"\"Custom analysis signature.\"\"\"\n    text_chunk = dspy.InputField(desc=\"Text to analyze\")\n    custom_errors = dspy.OutputField(desc=\"JSON list of custom errors\")\n\nclass CustomAnalyzer:\n    \"\"\"Custom analysis module.\"\"\"\n    \n    def __init__(self):\n        self.analyzer = dspy.Predict(CustomAnalysisSignature)\n    \n    def analyze(self, text_block: TextBlock) -&gt; List[BaseError]:\n        result = self.analyzer(text_chunk=text_block.content)\n        # Parse result.custom_errors and return BaseError objects\n        return self._parse_errors(result.custom_errors, text_block)\n\n\n\nAdd custom configuration options:\nfrom veritascribe.config import VeritaScribeSettings\nfrom pydantic import Field\n\nclass ExtendedSettings(VeritaScribeSettings):\n    \"\"\"Extended configuration with custom options.\"\"\"\n    \n    custom_analysis_enabled: bool = Field(\n        default=False,\n        description=\"Enable custom analysis module\"\n    )\n    \n    custom_threshold: float = Field(\n        default=0.7,\n        description=\"Custom analysis threshold\"\n    )"
  },
  {
    "objectID": "api-reference.html#testing-utilities",
    "href": "api-reference.html#testing-utilities",
    "title": "API Reference",
    "section": "",
    "text": "from veritascribe.testing import (\n    create_test_document,\n    create_test_errors,\n    mock_llm_response\n)\n\n# Create test documents\ntest_pdf = create_test_document(\n    content=\"Sample thesis content...\",\n    filename=\"test.pdf\"\n)\n\n# Create test errors\ntest_errors = create_test_errors(\n    count=5,\n    error_types=[\"grammar\", \"citation_format\"]\n)\n\n# Mock LLM responses for testing\nwith mock_llm_response(test_errors):\n    result = analyzer.analyze(text_block)\n\n\n\nimport pytest\nfrom veritascribe.testing import AnalysisFixture\n\n@pytest.fixture\ndef analysis_fixture():\n    return AnalysisFixture(\n        document_pages=10,\n        error_count=5,\n        include_visualizations=True\n    )\n\ndef test_analysis_pipeline(analysis_fixture):\n    report = analysis_fixture.run_analysis()\n    assert report.total_errors == 5"
  },
  {
    "objectID": "api-reference.html#performance-considerations",
    "href": "api-reference.html#performance-considerations",
    "title": "API Reference",
    "section": "",
    "text": "Batch Processing: Process multiple blocks efficiently\nCaching: Cache LLM responses when possible\nAsync Processing: Use parallel processing for large documents\nMemory Management: Handle large PDFs without memory issues\n\n\n\n\nfrom veritascribe.monitoring import (\n    track_token_usage,\n    measure_processing_time,\n    log_api_calls\n)\n\n@track_token_usage\n@measure_processing_time\ndef analyze_with_monitoring(text_block):\n    return analyzer.analyze(text_block)"
  },
  {
    "objectID": "api-reference.html#security-considerations",
    "href": "api-reference.html#security-considerations",
    "title": "API Reference",
    "section": "",
    "text": "Never log API keys\nUse environment variables in production\nRotate keys regularly\nMonitor usage for anomalies\n\n\n\n\nAll inputs are validated using Pydantic models:\nfrom pydantic import ValidationError\n\ntry:\n    settings = VeritaScribeSettings(temperature=2.0)  # Invalid\nexcept ValidationError as e:\n    print(\"Invalid temperature value\")\n\n\n\nReports are sanitized to prevent injection attacks:\nfrom veritascribe.security import sanitize_output\n\nsafe_content = sanitize_output(user_content)\n\nThis API reference covers the main interfaces. For implementation details, see the Architecture Guide."
  },
  {
    "objectID": "architecture.html",
    "href": "architecture.html",
    "title": "Architecture Guide",
    "section": "",
    "text": "This guide provides a comprehensive overview of VeritaScribe’s system architecture, design patterns, and implementation details for developers and contributors.\n\n\nVeritaScribe follows a modular pipeline architecture designed for scalability, maintainability, and extensibility.\n\n\n\n\n\ngraph TB\n    A[PDF Input] --&gt; B[PDF Processor]\n    B --&gt; C[Text Blocks]\n    C --&gt; D[Analysis Pipeline]\n    D --&gt; E[LLM Modules]\n    E --&gt; F[Error Detection]\n    F --&gt; G[Result Aggregation]\n    G --&gt; H[Report Generator]\n    H --&gt; I[Output Files]\n    \n    subgraph \"Configuration Layer\"\n        J[Environment Variables]\n        K[Settings Management]\n        L[DSPy Configuration]\n    end\n    \n    subgraph \"Data Models\"\n        M[Pydantic Schemas]\n        N[Error Types]\n        O[Validation]\n    end\n    \n    D --&gt; J\n    E --&gt; L\n    F --&gt; M\n\n\n\n\n\n\n\n\n\n\n\nThe configuration system provides centralized, type-safe settings management.\n\n\n\nEnvironment-first: Configuration comes from environment variables\nType safety: All settings validated with Pydantic\nHierarchical loading: Environment → .env → defaults\nImmutable: Settings loaded once and cached\n\n\n\n\nclass VeritaScribeSettings(BaseSettings):\n    \"\"\"Main configuration using pydantic-settings.\"\"\"\n    \n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        case_sensitive=False,\n        extra=\"ignore\"\n    )\nKey Features: - Automatic environment variable mapping - Validation on load with clear error messages - Support for development/production environments - Integration with DSPy LLM configuration\n\n\n\nclass DSPyConfig:\n    \"\"\"Manages DSPy LLM backend configuration.\"\"\"\n    \n    def initialize_llm(self) -&gt; dspy.LM:\n        \"\"\"Initialize and configure DSPy LLM backend.\"\"\"\n        self._lm = dspy.LM(\n            model=self.settings.default_model,\n            api_key=self.settings.openai_api_key,\n            max_tokens=self.settings.max_tokens,\n            temperature=self.settings.temperature\n        )\n        dspy.configure(lm=self._lm)\n        return self._lm\n\n\n\n\nThe data layer uses Pydantic for robust data validation and serialization.\n\n\n\nType safety: All data structures are strongly typed\nValidation: Input validation prevents runtime errors\nSerialization: Seamless JSON/dict conversion\nImmutability: Data models are read-only after creation\n\n\n\n\n\n\n\n\n\nclassDiagram\n    class LocationHint {\n        +int page_number\n        +List[float] bounding_box_coordinates\n    }\n    \n    class BaseError {\n        +str error_type\n        +ErrorSeverity severity\n        +str original_text\n        +str suggested_correction\n        +str explanation\n        +LocationHint location\n        +float confidence_score\n    }\n    \n    class TextBlock {\n        +str content\n        +int page_number\n        +List[float] bounding_box_coordinates\n    }\n    \n    class AnalysisResult {\n        +TextBlock text_block\n        +List[BaseError] errors\n    }\n    \n    class ThesisAnalysisReport {\n        +str document_name\n        +datetime analysis_timestamp\n        +int total_pages\n        +int total_words\n        +List[AnalysisResult] analysis_results\n        +float total_processing_time_seconds\n    }\n    \n    BaseError --&gt; LocationHint\n    AnalysisResult --&gt; TextBlock\n    AnalysisResult --&gt; BaseError\n    ThesisAnalysisReport --&gt; AnalysisResult\n\n\n\n\n\n\n\n\n\nclass ErrorSeverity(str, Enum):\n    \"\"\"Enumeration for error severity levels.\"\"\"\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n\nclass BaseError(BaseModel):\n    \"\"\"Base class for all error types with common fields.\"\"\"\n    error_type: str\n    severity: ErrorSeverity\n    original_text: str\n    suggested_correction: Optional[str] = None\n    explanation: str\n    location: LocationHint\n    confidence_score: float = Field(ge=0.0, le=1.0)\nExtensibility: New error types inherit from BaseError:\nclass GrammarError(BaseError):\n    \"\"\"Grammar and linguistic errors.\"\"\"\n    error_type: str = Field(default=\"grammar\", frozen=True)\n\nclass CitationError(BaseError):\n    \"\"\"Citation format and referencing errors.\"\"\"  \n    error_type: str = Field(default=\"citation_format\", frozen=True)\n    citation_style: Optional[str] = None\n\n\n\n\nThe PDF processing layer extracts text while preserving spatial and layout information.\n\n\n\nPyMuPDF (fitz): Core PDF processing library\nLayout preservation: Maintains bounding box coordinates\nPerformance: Efficient memory usage for large documents\n\n\n\n\nclass PDFProcessor:\n    \"\"\"Main PDF processing class.\"\"\"\n    \n    def extract_text_blocks_from_pdf(self, pdf_path: str) -&gt; List[TextBlock]:\n        \"\"\"Extract text blocks with layout information.\"\"\"\n        text_blocks = []\n        \n        with fitz.open(pdf_path) as doc:\n            for page_num, page in enumerate(doc, 1):\n                # Extract text blocks with coordinates\n                blocks = page.get_text(\"dict\")\n                \n                for block in blocks[\"blocks\"]:\n                    if \"lines\" in block:  # Text block\n                        text_block = self._process_text_block(\n                            block, page_num\n                        )\n                        if text_block:\n                            text_blocks.append(text_block)\n        \n        return text_blocks\n\n\n\nEach text block preserves its location:\nTextBlock(\n    content=\"The methodology employed...\",\n    page_number=5,\n    bounding_box_coordinates=[72.0, 200.0, 520.0, 280.0]\n)\nCoordinates format: [x1, y1, x2, y2] where: - (x1, y1): Top-left corner - (x2, y2): Bottom-right corner\n- Units: PDF points (72 points = 1 inch)\n\n\n\n\nThe analysis layer uses DSPy for structured LLM interactions.\n\n\nDSPy provides declarative LLM programming with strong typing and structured outputs.\nclass LinguisticAnalysisSignature(dspy.Signature):\n    \"\"\"Grammar and linguistic analysis signature.\"\"\"\n    text_chunk = dspy.InputField(\n        desc=\"Text to analyze for grammar and linguistic issues\"\n    )\n    grammar_errors = dspy.OutputField(\n        desc=\"JSON list of grammar errors found\"\n    )\n\n\n\nEach analysis type follows a consistent pattern:\nclass LinguisticAnalyzer(dspy.Module):\n    \"\"\"Grammar and linguistic analysis module.\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.analyzer = dspy.Predict(LinguisticAnalysisSignature)\n    \n    def forward(self, text_chunk: str) -&gt; str:\n        \"\"\"Analyze text for linguistic issues.\"\"\"\n        return self.analyzer(text_chunk=text_chunk)\n    \n    def analyze(self, text_block: TextBlock) -&gt; List[BaseError]:\n        \"\"\"Convert DSPy output to validated error objects.\"\"\"\n        result = self.forward(text_block.content)\n        return self._parse_and_validate_errors(\n            result.grammar_errors, \n            text_block\n        )\n\n\n\nLLM outputs are parsed and validated against Pydantic schemas:\ndef _parse_and_validate_errors(\n    self, \n    llm_output: str, \n    text_block: TextBlock\n) -&gt; List[BaseError]:\n    \"\"\"Parse LLM JSON output and validate against schemas.\"\"\"\n    try:\n        error_data = json.loads(llm_output)\n        errors = []\n        \n        for item in error_data:\n            # Inject location information\n            item[\"location\"] = LocationHint(\n                page_number=text_block.page_number,\n                bounding_box_coordinates=text_block.bounding_box_coordinates\n            )\n            \n            # Validate against schema\n            error = BaseError(**item)\n            errors.append(error)\n            \n        return errors\n        \n    except (json.JSONDecodeError, ValidationError) as e:\n        # Handle malformed LLM output\n        self._log_parsing_error(e, llm_output)\n        return []\n\n\n\n\nThe pipeline coordinates the entire analysis workflow.\n\n\n\n\n\n\n\nsequenceDiagram\n    participant P as Pipeline\n    participant PDF as PDFProcessor  \n    participant LLM as LLMModules\n    participant A as Aggregator\n    \n    P-&gt;&gt;PDF: extract_text_blocks_from_pdf()\n    PDF-&gt;&gt;P: List[TextBlock]\n    \n    loop For each TextBlock\n        P-&gt;&gt;LLM: analyze(text_block)\n        LLM-&gt;&gt;P: List[BaseError]\n        P-&gt;&gt;A: collect_errors()\n    end\n    \n    P-&gt;&gt;A: create_report()\n    A-&gt;&gt;P: ThesisAnalysisReport\n\n\n\n\n\n\n\n\n\nclass AnalysisPipeline:\n    \"\"\"Main analysis orchestration pipeline.\"\"\"\n    \n    def __init__(self):\n        self.pdf_processor = PDFProcessor()\n        self.linguistic_analyzer = LinguisticAnalyzer()\n        self.content_validator = ContentValidator()\n        self.citation_checker = CitationChecker()\n    \n    def analyze_thesis(\n        self, \n        pdf_path: str, \n        output_dir: str,\n        citation_style: str = \"APA\"\n    ) -&gt; ThesisAnalysisReport:\n        \"\"\"Perform complete thesis analysis.\"\"\"\n        \n        # Phase 1: Extract text blocks\n        text_blocks = self.pdf_processor.extract_text_blocks_from_pdf(pdf_path)\n        \n        # Phase 2: Analyze each block\n        analysis_results = []\n        for text_block in text_blocks:\n            errors = self._analyze_text_block(text_block, citation_style)\n            analysis_results.append(\n                AnalysisResult(text_block=text_block, errors=errors)\n            )\n        \n        # Phase 3: Create comprehensive report\n        return self._create_report(pdf_path, analysis_results)\n\n\n\nFor performance, the pipeline supports parallel analysis:\ndef _analyze_blocks_parallel(\n    self, \n    text_blocks: List[TextBlock]\n) -&gt; List[AnalysisResult]:\n    \"\"\"Analyze blocks in parallel using ThreadPoolExecutor.\"\"\"\n    \n    with ThreadPoolExecutor(\n        max_workers=self.settings.max_concurrent_requests\n    ) as executor:\n        futures = [\n            executor.submit(self._analyze_text_block, block)\n            for block in text_blocks\n        ]\n        \n        results = []\n        for future in as_completed(futures):\n            try:\n                result = future.result(timeout=30)\n                results.append(result)\n            except TimeoutError:\n                self._handle_timeout_error()\n        \n        return results\n\n\n\n\nThe reporting layer creates multiple output formats from analysis results.\n\n\nclass ReportGenerator:\n    \"\"\"Generate reports and visualizations from analysis results.\"\"\"\n    \n    def generate_text_report(self, report: ThesisAnalysisReport, output_path: str):\n        \"\"\"Generate comprehensive Markdown report.\"\"\"\n        \n    def export_json_report(self, report: ThesisAnalysisReport, output_path: str):\n        \"\"\"Export structured JSON data.\"\"\"\n        \n    def visualize_errors(self, report: ThesisAnalysisReport, output_dir: str):\n        \"\"\"Generate matplotlib/seaborn visualizations.\"\"\"\n\n\n\nUses matplotlib for chart generation:\ndef _create_error_type_chart(self, report: ThesisAnalysisReport) -&gt; str:\n    \"\"\"Create error distribution by type chart.\"\"\"\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    error_types = list(report.errors_by_type.keys())\n    error_counts = list(report.errors_by_type.values())\n    \n    ax.bar(error_types, error_counts)\n    ax.set_title(\"Error Distribution by Type\")\n    ax.set_xlabel(\"Error Type\")\n    ax.set_ylabel(\"Count\")\n    \n    # Save with consistent styling\n    plt.tight_layout()\n    chart_path = os.path.join(output_dir, \"error_types.png\")\n    plt.savefig(chart_path, dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    return chart_path\n\n\n\n\n\n\n\nUsed for creating configured instances:\ndef create_analysis_pipeline() -&gt; AnalysisPipeline:\n    \"\"\"Factory function for analysis pipeline.\"\"\"\n    settings, dspy_config = initialize_system()\n    return AnalysisPipeline(settings=settings, dspy_config=dspy_config)\n\n\n\nDifferent analysis strategies can be plugged in:\nclass AnalysisStrategy(ABC):\n    \"\"\"Abstract base for analysis strategies.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, text_block: TextBlock) -&gt; List[BaseError]:\n        pass\n\nclass GrammarAnalysisStrategy(AnalysisStrategy):\n    \"\"\"Grammar-focused analysis strategy.\"\"\"\n    \n    def analyze(self, text_block: TextBlock) -&gt; List[BaseError]:\n        # Grammar-specific analysis\n        pass\n\n\n\nFor monitoring and logging:\nclass AnalysisObserver(ABC):\n    \"\"\"Observer for analysis events.\"\"\"\n    \n    @abstractmethod\n    def on_block_analyzed(self, block: TextBlock, errors: List[BaseError]):\n        pass\n\nclass TokenUsageObserver(AnalysisObserver):\n    \"\"\"Track token usage during analysis.\"\"\"\n    \n    def on_block_analyzed(self, block: TextBlock, errors: List[BaseError]):\n        self.total_tokens += self._estimate_tokens(block.content)\n\n\n\n\n\n\nclass VeritaScribeError(Exception):\n    \"\"\"Base exception for all VeritaScribe errors.\"\"\"\n    pass\n\nclass ConfigurationError(VeritaScribeError):\n    \"\"\"Configuration-related errors.\"\"\"\n    pass\n\nclass PDFProcessingError(VeritaScribeError):\n    \"\"\"PDF processing errors.\"\"\"\n    pass\n\nclass LLMAnalysisError(VeritaScribeError):\n    \"\"\"LLM analysis errors.\"\"\"\n    \n    def __init__(self, message: str, context: Dict[str, Any] = None):\n        super().__init__(message)\n        self.context = context or {}\n\n\n\nclass RetryHandler:\n    \"\"\"Handle retries for transient failures.\"\"\"\n    \n    def __init__(self, max_retries: int = 3, delay: float = 1.0):\n        self.max_retries = max_retries\n        self.delay = delay\n    \n    def retry_with_backoff(self, func: Callable, *args, **kwargs):\n        \"\"\"Retry function with exponential backoff.\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                return func(*args, **kwargs)\n            except (APIError, TimeoutError) as e:\n                if attempt == self.max_retries - 1:\n                    raise\n                \n                sleep_time = self.delay * (2 ** attempt)\n                time.sleep(sleep_time)\n\n\n\ndef analyze_with_fallback(self, text_block: TextBlock) -&gt; List[BaseError]:\n    \"\"\"Analyze with fallback strategies.\"\"\"\n    try:\n        # Try primary analysis\n        return self.primary_analyzer.analyze(text_block)\n    except LLMAnalysisError:\n        try:\n            # Fallback to simpler analysis\n            return self.fallback_analyzer.analyze(text_block)\n        except Exception:\n            # Return empty results rather than failing\n            self._log_analysis_failure(text_block)\n            return []\n\n\n\n\n\n\nfrom functools import lru_cache\nimport hashlib\n\nclass CachedAnalyzer:\n    \"\"\"Analyzer with response caching.\"\"\"\n    \n    def __init__(self):\n        self.cache = {}\n    \n    def analyze(self, text_block: TextBlock) -&gt; List[BaseError]:\n        # Create cache key from content hash\n        content_hash = hashlib.md5(\n            text_block.content.encode()\n        ).hexdigest()\n        \n        if content_hash in self.cache:\n            return self.cache[content_hash]\n        \n        # Perform analysis\n        result = self._perform_analysis(text_block)\n        self.cache[content_hash] = result\n        return result\n\n\n\ndef process_large_document(self, pdf_path: str) -&gt; ThesisAnalysisReport:\n    \"\"\"Process large documents with memory optimization.\"\"\"\n    \n    # Process in chunks to avoid memory issues\n    chunk_size = self.settings.max_text_block_size\n    results = []\n    \n    for chunk in self._chunk_document(pdf_path, chunk_size):\n        # Process chunk\n        chunk_results = self._process_chunk(chunk)\n        results.extend(chunk_results)\n        \n        # Clear intermediate data\n        gc.collect()\n    \n    return self._aggregate_results(results)\n\n\n\nimport asyncio\nimport aiofiles\n\nclass AsyncAnalysisPipeline:\n    \"\"\"Async version of analysis pipeline.\"\"\"\n    \n    async def analyze_thesis_async(\n        self, \n        pdf_path: str\n    ) -&gt; ThesisAnalysisReport:\n        \"\"\"Async analysis with concurrency control.\"\"\"\n        \n        text_blocks = await self._extract_text_blocks_async(pdf_path)\n        \n        # Create semaphore for concurrency control\n        semaphore = asyncio.Semaphore(\n            self.settings.max_concurrent_requests\n        )\n        \n        # Process blocks concurrently\n        tasks = [\n            self._analyze_block_async(block, semaphore)\n            for block in text_blocks\n        ]\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return self._create_report(pdf_path, results)\n\n\n\n\n\n\nAll inputs are validated at multiple layers:\ndef validate_pdf_path(pdf_path: str) -&gt; Path:\n    \"\"\"Validate PDF path for security.\"\"\"\n    path = Path(pdf_path).resolve()\n    \n    # Check file exists and is readable\n    if not path.exists():\n        raise FileNotFoundError(f\"PDF not found: {path}\")\n    \n    # Check file extension\n    if path.suffix.lower() != '.pdf':\n        raise ValueError(f\"File must be PDF: {path}\")\n    \n    # Check file size (prevent DoS)\n    if path.stat().st_size &gt; MAX_FILE_SIZE:\n        raise ValueError(f\"File too large: {path}\")\n    \n    return path\n\n\n\nclass SecureSettings(VeritaScribeSettings):\n    \"\"\"Settings with enhanced security.\"\"\"\n    \n    @field_validator('openai_api_key')\n    def validate_api_key(cls, v):\n        if v and len(v) &lt; 10:  # Basic validation\n            raise ValueError(\"Invalid API key format\")\n        return v\n    \n    def __repr__(self):\n        # Never expose API key in logs\n        safe_dict = self.model_dump()\n        if 'openai_api_key' in safe_dict:\n            safe_dict['openai_api_key'] = '[REDACTED]'\n        return f\"Settings({safe_dict})\"\n\n\n\ndef sanitize_output(content: str) -&gt; str:\n    \"\"\"Sanitize content for safe output.\"\"\"\n    # Remove potential script injection\n    content = re.sub(r'&lt;script.*?&lt;/script&gt;', '', content, flags=re.IGNORECASE)\n    \n    # Escape HTML entities\n    content = html.escape(content)\n    \n    # Limit output length\n    if len(content) &gt; MAX_OUTPUT_LENGTH:\n        content = content[:MAX_OUTPUT_LENGTH] + \"...\"\n    \n    return content\n\n\n\n\n\n\ntests/\n├── unit/                    # Unit tests for individual components\n│   ├── test_config.py\n│   ├── test_data_models.py\n│   ├── test_pdf_processor.py\n│   └── test_llm_modules.py\n├── integration/             # Integration tests\n│   ├── test_pipeline.py\n│   └── test_report_generation.py\n├── fixtures/                # Test data and fixtures\n│   ├── sample_pdfs/\n│   └── expected_outputs/\n└── conftest.py             # Pytest configuration\n\n\n\nimport pytest\nfrom unittest.mock import Mock, patch\n\n@pytest.fixture\ndef mock_llm():\n    \"\"\"Mock LLM for testing without API calls.\"\"\"\n    with patch('dspy.LM') as mock:\n        mock_instance = Mock()\n        mock_instance.generate.return_value = MockResponse(\n            text='[{\"error_type\": \"grammar\", \"severity\": \"high\", ...}]'\n        )\n        mock.return_value = mock_instance\n        yield mock_instance\n\ndef test_linguistic_analyzer(mock_llm):\n    \"\"\"Test analyzer with mocked LLM.\"\"\"\n    analyzer = LinguisticAnalyzer()\n    text_block = TextBlock(content=\"Test content\", page_number=1)\n    \n    errors = analyzer.analyze(text_block)\n    \n    assert len(errors) &gt; 0\n    assert errors[0].error_type == \"grammar\"\n\n\n\nfrom hypothesis import given, strategies as st\n\n@given(\n    content=st.text(min_size=10, max_size=1000),\n    page_number=st.integers(min_value=1, max_value=100)\n)\ndef test_text_block_creation(content, page_number):\n    \"\"\"Property-based test for TextBlock creation.\"\"\"\n    block = TextBlock(content=content, page_number=page_number)\n    \n    assert block.content == content\n    assert block.page_number == page_number\n    assert len(block.content) &gt;= 10\n\n\n\n\n\n\nFROM python:3.13-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nCOPY pyproject.toml uv.lock ./\nRUN pip install uv && uv sync --frozen\n\n# Copy application code\nCOPY src/ ./src/\n\n# Set environment variables\nENV PYTHONPATH=/app/src\nENV OPENAI_API_KEY=\"\"\n\n# Run application\nCMD [\"uv\", \"run\", \"python\", \"-m\", \"veritascribe\"]\n\n\n\n# docker-compose.prod.yml\nversion: '3.8'\nservices:\n  veritascribe:\n    build: .\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - DEFAULT_MODEL=gpt-4-turbo\n      - MAX_CONCURRENT_REQUESTS=10\n      - PARALLEL_PROCESSING=true\n      - OUTPUT_DIRECTORY=/app/output\n    volumes:\n      - ./input:/app/input:ro\n      - ./output:/app/output\n      - ./logs:/app/logs\n    restart: unless-stopped\n    resource_limits:\n      cpus: '2'\n      memory: 4G\n\n\n\nimport structlog\nfrom prometheus_client import Counter, Histogram, start_http_server\n\n# Metrics\nANALYSIS_COUNTER = Counter('veritascribe_analyses_total', 'Total analyses')\nPROCESSING_TIME = Histogram('veritascribe_processing_seconds', 'Processing time')\nERROR_COUNTER = Counter('veritascribe_errors_total', 'Total errors', ['error_type'])\n\nlogger = structlog.get_logger()\n\nclass InstrumentedPipeline(AnalysisPipeline):\n    \"\"\"Pipeline with monitoring instrumentation.\"\"\"\n    \n    @PROCESSING_TIME.time()\n    def analyze_thesis(self, pdf_path: str, **kwargs) -&gt; ThesisAnalysisReport:\n        \"\"\"Instrumented analysis with metrics.\"\"\"\n        ANALYSIS_COUNTER.inc()\n        \n        logger.info(\"Starting analysis\", pdf_path=pdf_path)\n        \n        try:\n            report = super().analyze_thesis(pdf_path, **kwargs)\n            \n            # Record metrics\n            for error_type, count in report.errors_by_type.items():\n                ERROR_COUNTER.labels(error_type=error_type).inc(count)\n            \n            logger.info(\n                \"Analysis completed\", \n                pdf_path=pdf_path,\n                total_errors=report.total_errors,\n                processing_time=report.total_processing_time_seconds\n            )\n            \n            return report\n            \n        except Exception as e:\n            logger.error(\"Analysis failed\", pdf_path=pdf_path, error=str(e))\n            raise\n\nThis architecture guide provides the foundation for understanding and extending VeritaScribe. For specific implementation details, refer to the source code and API Reference."
  },
  {
    "objectID": "architecture.html#system-overview",
    "href": "architecture.html#system-overview",
    "title": "Architecture Guide",
    "section": "",
    "text": "VeritaScribe follows a modular pipeline architecture designed for scalability, maintainability, and extensibility.\n\n\n\n\n\ngraph TB\n    A[PDF Input] --&gt; B[PDF Processor]\n    B --&gt; C[Text Blocks]\n    C --&gt; D[Analysis Pipeline]\n    D --&gt; E[LLM Modules]\n    E --&gt; F[Error Detection]\n    F --&gt; G[Result Aggregation]\n    G --&gt; H[Report Generator]\n    H --&gt; I[Output Files]\n    \n    subgraph \"Configuration Layer\"\n        J[Environment Variables]\n        K[Settings Management]\n        L[DSPy Configuration]\n    end\n    \n    subgraph \"Data Models\"\n        M[Pydantic Schemas]\n        N[Error Types]\n        O[Validation]\n    end\n    \n    D --&gt; J\n    E --&gt; L\n    F --&gt; M"
  },
  {
    "objectID": "architecture.html#core-components",
    "href": "architecture.html#core-components",
    "title": "Architecture Guide",
    "section": "",
    "text": "The configuration system provides centralized, type-safe settings management.\n\n\n\nEnvironment-first: Configuration comes from environment variables\nType safety: All settings validated with Pydantic\nHierarchical loading: Environment → .env → defaults\nImmutable: Settings loaded once and cached\n\n\n\n\nclass VeritaScribeSettings(BaseSettings):\n    \"\"\"Main configuration using pydantic-settings.\"\"\"\n    \n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        case_sensitive=False,\n        extra=\"ignore\"\n    )\nKey Features: - Automatic environment variable mapping - Validation on load with clear error messages - Support for development/production environments - Integration with DSPy LLM configuration\n\n\n\nclass DSPyConfig:\n    \"\"\"Manages DSPy LLM backend configuration.\"\"\"\n    \n    def initialize_llm(self) -&gt; dspy.LM:\n        \"\"\"Initialize and configure DSPy LLM backend.\"\"\"\n        self._lm = dspy.LM(\n            model=self.settings.default_model,\n            api_key=self.settings.openai_api_key,\n            max_tokens=self.settings.max_tokens,\n            temperature=self.settings.temperature\n        )\n        dspy.configure(lm=self._lm)\n        return self._lm\n\n\n\n\nThe data layer uses Pydantic for robust data validation and serialization.\n\n\n\nType safety: All data structures are strongly typed\nValidation: Input validation prevents runtime errors\nSerialization: Seamless JSON/dict conversion\nImmutability: Data models are read-only after creation\n\n\n\n\n\n\n\n\n\nclassDiagram\n    class LocationHint {\n        +int page_number\n        +List[float] bounding_box_coordinates\n    }\n    \n    class BaseError {\n        +str error_type\n        +ErrorSeverity severity\n        +str original_text\n        +str suggested_correction\n        +str explanation\n        +LocationHint location\n        +float confidence_score\n    }\n    \n    class TextBlock {\n        +str content\n        +int page_number\n        +List[float] bounding_box_coordinates\n    }\n    \n    class AnalysisResult {\n        +TextBlock text_block\n        +List[BaseError] errors\n    }\n    \n    class ThesisAnalysisReport {\n        +str document_name\n        +datetime analysis_timestamp\n        +int total_pages\n        +int total_words\n        +List[AnalysisResult] analysis_results\n        +float total_processing_time_seconds\n    }\n    \n    BaseError --&gt; LocationHint\n    AnalysisResult --&gt; TextBlock\n    AnalysisResult --&gt; BaseError\n    ThesisAnalysisReport --&gt; AnalysisResult\n\n\n\n\n\n\n\n\n\nclass ErrorSeverity(str, Enum):\n    \"\"\"Enumeration for error severity levels.\"\"\"\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n\nclass BaseError(BaseModel):\n    \"\"\"Base class for all error types with common fields.\"\"\"\n    error_type: str\n    severity: ErrorSeverity\n    original_text: str\n    suggested_correction: Optional[str] = None\n    explanation: str\n    location: LocationHint\n    confidence_score: float = Field(ge=0.0, le=1.0)\nExtensibility: New error types inherit from BaseError:\nclass GrammarError(BaseError):\n    \"\"\"Grammar and linguistic errors.\"\"\"\n    error_type: str = Field(default=\"grammar\", frozen=True)\n\nclass CitationError(BaseError):\n    \"\"\"Citation format and referencing errors.\"\"\"  \n    error_type: str = Field(default=\"citation_format\", frozen=True)\n    citation_style: Optional[str] = None\n\n\n\n\nThe PDF processing layer extracts text while preserving spatial and layout information.\n\n\n\nPyMuPDF (fitz): Core PDF processing library\nLayout preservation: Maintains bounding box coordinates\nPerformance: Efficient memory usage for large documents\n\n\n\n\nclass PDFProcessor:\n    \"\"\"Main PDF processing class.\"\"\"\n    \n    def extract_text_blocks_from_pdf(self, pdf_path: str) -&gt; List[TextBlock]:\n        \"\"\"Extract text blocks with layout information.\"\"\"\n        text_blocks = []\n        \n        with fitz.open(pdf_path) as doc:\n            for page_num, page in enumerate(doc, 1):\n                # Extract text blocks with coordinates\n                blocks = page.get_text(\"dict\")\n                \n                for block in blocks[\"blocks\"]:\n                    if \"lines\" in block:  # Text block\n                        text_block = self._process_text_block(\n                            block, page_num\n                        )\n                        if text_block:\n                            text_blocks.append(text_block)\n        \n        return text_blocks\n\n\n\nEach text block preserves its location:\nTextBlock(\n    content=\"The methodology employed...\",\n    page_number=5,\n    bounding_box_coordinates=[72.0, 200.0, 520.0, 280.0]\n)\nCoordinates format: [x1, y1, x2, y2] where: - (x1, y1): Top-left corner - (x2, y2): Bottom-right corner\n- Units: PDF points (72 points = 1 inch)\n\n\n\n\nThe analysis layer uses DSPy for structured LLM interactions.\n\n\nDSPy provides declarative LLM programming with strong typing and structured outputs.\nclass LinguisticAnalysisSignature(dspy.Signature):\n    \"\"\"Grammar and linguistic analysis signature.\"\"\"\n    text_chunk = dspy.InputField(\n        desc=\"Text to analyze for grammar and linguistic issues\"\n    )\n    grammar_errors = dspy.OutputField(\n        desc=\"JSON list of grammar errors found\"\n    )\n\n\n\nEach analysis type follows a consistent pattern:\nclass LinguisticAnalyzer(dspy.Module):\n    \"\"\"Grammar and linguistic analysis module.\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.analyzer = dspy.Predict(LinguisticAnalysisSignature)\n    \n    def forward(self, text_chunk: str) -&gt; str:\n        \"\"\"Analyze text for linguistic issues.\"\"\"\n        return self.analyzer(text_chunk=text_chunk)\n    \n    def analyze(self, text_block: TextBlock) -&gt; List[BaseError]:\n        \"\"\"Convert DSPy output to validated error objects.\"\"\"\n        result = self.forward(text_block.content)\n        return self._parse_and_validate_errors(\n            result.grammar_errors, \n            text_block\n        )\n\n\n\nLLM outputs are parsed and validated against Pydantic schemas:\ndef _parse_and_validate_errors(\n    self, \n    llm_output: str, \n    text_block: TextBlock\n) -&gt; List[BaseError]:\n    \"\"\"Parse LLM JSON output and validate against schemas.\"\"\"\n    try:\n        error_data = json.loads(llm_output)\n        errors = []\n        \n        for item in error_data:\n            # Inject location information\n            item[\"location\"] = LocationHint(\n                page_number=text_block.page_number,\n                bounding_box_coordinates=text_block.bounding_box_coordinates\n            )\n            \n            # Validate against schema\n            error = BaseError(**item)\n            errors.append(error)\n            \n        return errors\n        \n    except (json.JSONDecodeError, ValidationError) as e:\n        # Handle malformed LLM output\n        self._log_parsing_error(e, llm_output)\n        return []\n\n\n\n\nThe pipeline coordinates the entire analysis workflow.\n\n\n\n\n\n\n\nsequenceDiagram\n    participant P as Pipeline\n    participant PDF as PDFProcessor  \n    participant LLM as LLMModules\n    participant A as Aggregator\n    \n    P-&gt;&gt;PDF: extract_text_blocks_from_pdf()\n    PDF-&gt;&gt;P: List[TextBlock]\n    \n    loop For each TextBlock\n        P-&gt;&gt;LLM: analyze(text_block)\n        LLM-&gt;&gt;P: List[BaseError]\n        P-&gt;&gt;A: collect_errors()\n    end\n    \n    P-&gt;&gt;A: create_report()\n    A-&gt;&gt;P: ThesisAnalysisReport\n\n\n\n\n\n\n\n\n\nclass AnalysisPipeline:\n    \"\"\"Main analysis orchestration pipeline.\"\"\"\n    \n    def __init__(self):\n        self.pdf_processor = PDFProcessor()\n        self.linguistic_analyzer = LinguisticAnalyzer()\n        self.content_validator = ContentValidator()\n        self.citation_checker = CitationChecker()\n    \n    def analyze_thesis(\n        self, \n        pdf_path: str, \n        output_dir: str,\n        citation_style: str = \"APA\"\n    ) -&gt; ThesisAnalysisReport:\n        \"\"\"Perform complete thesis analysis.\"\"\"\n        \n        # Phase 1: Extract text blocks\n        text_blocks = self.pdf_processor.extract_text_blocks_from_pdf(pdf_path)\n        \n        # Phase 2: Analyze each block\n        analysis_results = []\n        for text_block in text_blocks:\n            errors = self._analyze_text_block(text_block, citation_style)\n            analysis_results.append(\n                AnalysisResult(text_block=text_block, errors=errors)\n            )\n        \n        # Phase 3: Create comprehensive report\n        return self._create_report(pdf_path, analysis_results)\n\n\n\nFor performance, the pipeline supports parallel analysis:\ndef _analyze_blocks_parallel(\n    self, \n    text_blocks: List[TextBlock]\n) -&gt; List[AnalysisResult]:\n    \"\"\"Analyze blocks in parallel using ThreadPoolExecutor.\"\"\"\n    \n    with ThreadPoolExecutor(\n        max_workers=self.settings.max_concurrent_requests\n    ) as executor:\n        futures = [\n            executor.submit(self._analyze_text_block, block)\n            for block in text_blocks\n        ]\n        \n        results = []\n        for future in as_completed(futures):\n            try:\n                result = future.result(timeout=30)\n                results.append(result)\n            except TimeoutError:\n                self._handle_timeout_error()\n        \n        return results\n\n\n\n\nThe reporting layer creates multiple output formats from analysis results.\n\n\nclass ReportGenerator:\n    \"\"\"Generate reports and visualizations from analysis results.\"\"\"\n    \n    def generate_text_report(self, report: ThesisAnalysisReport, output_path: str):\n        \"\"\"Generate comprehensive Markdown report.\"\"\"\n        \n    def export_json_report(self, report: ThesisAnalysisReport, output_path: str):\n        \"\"\"Export structured JSON data.\"\"\"\n        \n    def visualize_errors(self, report: ThesisAnalysisReport, output_dir: str):\n        \"\"\"Generate matplotlib/seaborn visualizations.\"\"\"\n\n\n\nUses matplotlib for chart generation:\ndef _create_error_type_chart(self, report: ThesisAnalysisReport) -&gt; str:\n    \"\"\"Create error distribution by type chart.\"\"\"\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    error_types = list(report.errors_by_type.keys())\n    error_counts = list(report.errors_by_type.values())\n    \n    ax.bar(error_types, error_counts)\n    ax.set_title(\"Error Distribution by Type\")\n    ax.set_xlabel(\"Error Type\")\n    ax.set_ylabel(\"Count\")\n    \n    # Save with consistent styling\n    plt.tight_layout()\n    chart_path = os.path.join(output_dir, \"error_types.png\")\n    plt.savefig(chart_path, dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    return chart_path"
  },
  {
    "objectID": "architecture.html#design-patterns",
    "href": "architecture.html#design-patterns",
    "title": "Architecture Guide",
    "section": "",
    "text": "Used for creating configured instances:\ndef create_analysis_pipeline() -&gt; AnalysisPipeline:\n    \"\"\"Factory function for analysis pipeline.\"\"\"\n    settings, dspy_config = initialize_system()\n    return AnalysisPipeline(settings=settings, dspy_config=dspy_config)\n\n\n\nDifferent analysis strategies can be plugged in:\nclass AnalysisStrategy(ABC):\n    \"\"\"Abstract base for analysis strategies.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, text_block: TextBlock) -&gt; List[BaseError]:\n        pass\n\nclass GrammarAnalysisStrategy(AnalysisStrategy):\n    \"\"\"Grammar-focused analysis strategy.\"\"\"\n    \n    def analyze(self, text_block: TextBlock) -&gt; List[BaseError]:\n        # Grammar-specific analysis\n        pass\n\n\n\nFor monitoring and logging:\nclass AnalysisObserver(ABC):\n    \"\"\"Observer for analysis events.\"\"\"\n    \n    @abstractmethod\n    def on_block_analyzed(self, block: TextBlock, errors: List[BaseError]):\n        pass\n\nclass TokenUsageObserver(AnalysisObserver):\n    \"\"\"Track token usage during analysis.\"\"\"\n    \n    def on_block_analyzed(self, block: TextBlock, errors: List[BaseError]):\n        self.total_tokens += self._estimate_tokens(block.content)"
  },
  {
    "objectID": "architecture.html#error-handling-strategy",
    "href": "architecture.html#error-handling-strategy",
    "title": "Architecture Guide",
    "section": "",
    "text": "class VeritaScribeError(Exception):\n    \"\"\"Base exception for all VeritaScribe errors.\"\"\"\n    pass\n\nclass ConfigurationError(VeritaScribeError):\n    \"\"\"Configuration-related errors.\"\"\"\n    pass\n\nclass PDFProcessingError(VeritaScribeError):\n    \"\"\"PDF processing errors.\"\"\"\n    pass\n\nclass LLMAnalysisError(VeritaScribeError):\n    \"\"\"LLM analysis errors.\"\"\"\n    \n    def __init__(self, message: str, context: Dict[str, Any] = None):\n        super().__init__(message)\n        self.context = context or {}\n\n\n\nclass RetryHandler:\n    \"\"\"Handle retries for transient failures.\"\"\"\n    \n    def __init__(self, max_retries: int = 3, delay: float = 1.0):\n        self.max_retries = max_retries\n        self.delay = delay\n    \n    def retry_with_backoff(self, func: Callable, *args, **kwargs):\n        \"\"\"Retry function with exponential backoff.\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                return func(*args, **kwargs)\n            except (APIError, TimeoutError) as e:\n                if attempt == self.max_retries - 1:\n                    raise\n                \n                sleep_time = self.delay * (2 ** attempt)\n                time.sleep(sleep_time)\n\n\n\ndef analyze_with_fallback(self, text_block: TextBlock) -&gt; List[BaseError]:\n    \"\"\"Analyze with fallback strategies.\"\"\"\n    try:\n        # Try primary analysis\n        return self.primary_analyzer.analyze(text_block)\n    except LLMAnalysisError:\n        try:\n            # Fallback to simpler analysis\n            return self.fallback_analyzer.analyze(text_block)\n        except Exception:\n            # Return empty results rather than failing\n            self._log_analysis_failure(text_block)\n            return []"
  },
  {
    "objectID": "architecture.html#performance-optimization",
    "href": "architecture.html#performance-optimization",
    "title": "Architecture Guide",
    "section": "",
    "text": "from functools import lru_cache\nimport hashlib\n\nclass CachedAnalyzer:\n    \"\"\"Analyzer with response caching.\"\"\"\n    \n    def __init__(self):\n        self.cache = {}\n    \n    def analyze(self, text_block: TextBlock) -&gt; List[BaseError]:\n        # Create cache key from content hash\n        content_hash = hashlib.md5(\n            text_block.content.encode()\n        ).hexdigest()\n        \n        if content_hash in self.cache:\n            return self.cache[content_hash]\n        \n        # Perform analysis\n        result = self._perform_analysis(text_block)\n        self.cache[content_hash] = result\n        return result\n\n\n\ndef process_large_document(self, pdf_path: str) -&gt; ThesisAnalysisReport:\n    \"\"\"Process large documents with memory optimization.\"\"\"\n    \n    # Process in chunks to avoid memory issues\n    chunk_size = self.settings.max_text_block_size\n    results = []\n    \n    for chunk in self._chunk_document(pdf_path, chunk_size):\n        # Process chunk\n        chunk_results = self._process_chunk(chunk)\n        results.extend(chunk_results)\n        \n        # Clear intermediate data\n        gc.collect()\n    \n    return self._aggregate_results(results)\n\n\n\nimport asyncio\nimport aiofiles\n\nclass AsyncAnalysisPipeline:\n    \"\"\"Async version of analysis pipeline.\"\"\"\n    \n    async def analyze_thesis_async(\n        self, \n        pdf_path: str\n    ) -&gt; ThesisAnalysisReport:\n        \"\"\"Async analysis with concurrency control.\"\"\"\n        \n        text_blocks = await self._extract_text_blocks_async(pdf_path)\n        \n        # Create semaphore for concurrency control\n        semaphore = asyncio.Semaphore(\n            self.settings.max_concurrent_requests\n        )\n        \n        # Process blocks concurrently\n        tasks = [\n            self._analyze_block_async(block, semaphore)\n            for block in text_blocks\n        ]\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return self._create_report(pdf_path, results)"
  },
  {
    "objectID": "architecture.html#security-considerations",
    "href": "architecture.html#security-considerations",
    "title": "Architecture Guide",
    "section": "",
    "text": "All inputs are validated at multiple layers:\ndef validate_pdf_path(pdf_path: str) -&gt; Path:\n    \"\"\"Validate PDF path for security.\"\"\"\n    path = Path(pdf_path).resolve()\n    \n    # Check file exists and is readable\n    if not path.exists():\n        raise FileNotFoundError(f\"PDF not found: {path}\")\n    \n    # Check file extension\n    if path.suffix.lower() != '.pdf':\n        raise ValueError(f\"File must be PDF: {path}\")\n    \n    # Check file size (prevent DoS)\n    if path.stat().st_size &gt; MAX_FILE_SIZE:\n        raise ValueError(f\"File too large: {path}\")\n    \n    return path\n\n\n\nclass SecureSettings(VeritaScribeSettings):\n    \"\"\"Settings with enhanced security.\"\"\"\n    \n    @field_validator('openai_api_key')\n    def validate_api_key(cls, v):\n        if v and len(v) &lt; 10:  # Basic validation\n            raise ValueError(\"Invalid API key format\")\n        return v\n    \n    def __repr__(self):\n        # Never expose API key in logs\n        safe_dict = self.model_dump()\n        if 'openai_api_key' in safe_dict:\n            safe_dict['openai_api_key'] = '[REDACTED]'\n        return f\"Settings({safe_dict})\"\n\n\n\ndef sanitize_output(content: str) -&gt; str:\n    \"\"\"Sanitize content for safe output.\"\"\"\n    # Remove potential script injection\n    content = re.sub(r'&lt;script.*?&lt;/script&gt;', '', content, flags=re.IGNORECASE)\n    \n    # Escape HTML entities\n    content = html.escape(content)\n    \n    # Limit output length\n    if len(content) &gt; MAX_OUTPUT_LENGTH:\n        content = content[:MAX_OUTPUT_LENGTH] + \"...\"\n    \n    return content"
  },
  {
    "objectID": "architecture.html#testing-architecture",
    "href": "architecture.html#testing-architecture",
    "title": "Architecture Guide",
    "section": "",
    "text": "tests/\n├── unit/                    # Unit tests for individual components\n│   ├── test_config.py\n│   ├── test_data_models.py\n│   ├── test_pdf_processor.py\n│   └── test_llm_modules.py\n├── integration/             # Integration tests\n│   ├── test_pipeline.py\n│   └── test_report_generation.py\n├── fixtures/                # Test data and fixtures\n│   ├── sample_pdfs/\n│   └── expected_outputs/\n└── conftest.py             # Pytest configuration\n\n\n\nimport pytest\nfrom unittest.mock import Mock, patch\n\n@pytest.fixture\ndef mock_llm():\n    \"\"\"Mock LLM for testing without API calls.\"\"\"\n    with patch('dspy.LM') as mock:\n        mock_instance = Mock()\n        mock_instance.generate.return_value = MockResponse(\n            text='[{\"error_type\": \"grammar\", \"severity\": \"high\", ...}]'\n        )\n        mock.return_value = mock_instance\n        yield mock_instance\n\ndef test_linguistic_analyzer(mock_llm):\n    \"\"\"Test analyzer with mocked LLM.\"\"\"\n    analyzer = LinguisticAnalyzer()\n    text_block = TextBlock(content=\"Test content\", page_number=1)\n    \n    errors = analyzer.analyze(text_block)\n    \n    assert len(errors) &gt; 0\n    assert errors[0].error_type == \"grammar\"\n\n\n\nfrom hypothesis import given, strategies as st\n\n@given(\n    content=st.text(min_size=10, max_size=1000),\n    page_number=st.integers(min_value=1, max_value=100)\n)\ndef test_text_block_creation(content, page_number):\n    \"\"\"Property-based test for TextBlock creation.\"\"\"\n    block = TextBlock(content=content, page_number=page_number)\n    \n    assert block.content == content\n    assert block.page_number == page_number\n    assert len(block.content) &gt;= 10"
  },
  {
    "objectID": "architecture.html#deployment-considerations",
    "href": "architecture.html#deployment-considerations",
    "title": "Architecture Guide",
    "section": "",
    "text": "FROM python:3.13-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nCOPY pyproject.toml uv.lock ./\nRUN pip install uv && uv sync --frozen\n\n# Copy application code\nCOPY src/ ./src/\n\n# Set environment variables\nENV PYTHONPATH=/app/src\nENV OPENAI_API_KEY=\"\"\n\n# Run application\nCMD [\"uv\", \"run\", \"python\", \"-m\", \"veritascribe\"]\n\n\n\n# docker-compose.prod.yml\nversion: '3.8'\nservices:\n  veritascribe:\n    build: .\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - DEFAULT_MODEL=gpt-4-turbo\n      - MAX_CONCURRENT_REQUESTS=10\n      - PARALLEL_PROCESSING=true\n      - OUTPUT_DIRECTORY=/app/output\n    volumes:\n      - ./input:/app/input:ro\n      - ./output:/app/output\n      - ./logs:/app/logs\n    restart: unless-stopped\n    resource_limits:\n      cpus: '2'\n      memory: 4G\n\n\n\nimport structlog\nfrom prometheus_client import Counter, Histogram, start_http_server\n\n# Metrics\nANALYSIS_COUNTER = Counter('veritascribe_analyses_total', 'Total analyses')\nPROCESSING_TIME = Histogram('veritascribe_processing_seconds', 'Processing time')\nERROR_COUNTER = Counter('veritascribe_errors_total', 'Total errors', ['error_type'])\n\nlogger = structlog.get_logger()\n\nclass InstrumentedPipeline(AnalysisPipeline):\n    \"\"\"Pipeline with monitoring instrumentation.\"\"\"\n    \n    @PROCESSING_TIME.time()\n    def analyze_thesis(self, pdf_path: str, **kwargs) -&gt; ThesisAnalysisReport:\n        \"\"\"Instrumented analysis with metrics.\"\"\"\n        ANALYSIS_COUNTER.inc()\n        \n        logger.info(\"Starting analysis\", pdf_path=pdf_path)\n        \n        try:\n            report = super().analyze_thesis(pdf_path, **kwargs)\n            \n            # Record metrics\n            for error_type, count in report.errors_by_type.items():\n                ERROR_COUNTER.labels(error_type=error_type).inc(count)\n            \n            logger.info(\n                \"Analysis completed\", \n                pdf_path=pdf_path,\n                total_errors=report.total_errors,\n                processing_time=report.total_processing_time_seconds\n            )\n            \n            return report\n            \n        except Exception as e:\n            logger.error(\"Analysis failed\", pdf_path=pdf_path, error=str(e))\n            raise\n\nThis architecture guide provides the foundation for understanding and extending VeritaScribe. For specific implementation details, refer to the source code and API Reference."
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation Guide",
    "section": "",
    "text": "This guide walks you through installing and setting up VeritaScribe on your system.\n\n\nBefore installing VeritaScribe, ensure you have:\n\nPython 3.13 or higher: VeritaScribe requires modern Python features\nOpenAI API Key: Required for LLM-based analysis\nuv: Modern Python package manager (recommended)\n\n\n\n\nVerify you have Python 3.13 or higher:\npython --version\n# Should show Python 3.13.x or higher\nIf you need to install or upgrade Python:\n\nmacOSUbuntu/DebianWindows\n\n\n# Using Homebrew\nbrew install python@3.13\n\n# Using pyenv\npyenv install 3.13.0\npyenv global 3.13.0\n\n\n# Add deadsnakes PPA for latest Python versions\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt update\nsudo apt install python3.13 python3.13-venv python3.13-pip\n\n\nDownload Python 3.13 from python.org or use:\n# Using Chocolatey\nchoco install python --version=3.13.0\n\n# Using winget\nwinget install Python.Python.3.13\n\n\n\n\n\n\nuv is a fast Python package manager that VeritaScribe uses for dependency management:\n\nmacOS/LinuxWindowsAlternative (pip)\n\n\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n\npip install uv\n\n\n\n\n\n\nClone the VeritaScribe repository:\ngit clone &lt;repository-url&gt;\ncd VeritaScribe\n\n\n\nInstall all required dependencies using uv:\nuv sync\nThis will: - Create a virtual environment automatically - Install all dependencies specified in pyproject.toml - Set up the project for development\n\n\n\n\n\nCopy the example environment configuration:\ncp .env.example .env\n\n\n\nOpen .env in your preferred text editor and add your OpenAI API key:\n# Required: OpenAI API Configuration\nOPENAI_API_KEY=your_openai_api_key_here\n\n# Optional: Customize other settings as needed\nDEFAULT_MODEL=gpt-4\nMAX_TOKENS=2000\nTEMPERATURE=0.1\n\n\n\n\n\n\nOpenAI API Key Required\n\n\n\nVeritaScribe requires an OpenAI API key to function. You can obtain one from the OpenAI Platform.\nSecurity Note: Never commit your API key to version control. The .env file is already included in .gitignore.\n\n\n\n\n\n\nTest your installation with the built-in system test:\nuv run python -m veritascribe test\nThis will verify: - ✅ Configuration loading - ✅ PDF processing capabilities - ✅ LLM connectivity (if API key is configured)\nYou should see output similar to:\nRunning VeritaScribe system tests...\n✓ Configuration loading works\n✓ PDF processing works\n✓ Analysis modules work\n\nTest Results: 3/3 passed\n🎉 All tests passed! VeritaScribe is ready to use.\n\n\n\nRun the demo to see VeritaScribe in action:\nuv run python -m veritascribe demo\nThis will: 1. Create a sample thesis PDF 2. Perform a quick analysis 3. Display results and save example reports\n\n\n\n\n\nIf you prefer to use pip directly:\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -e .\n\n\n\nFor development work, install with additional dev dependencies:\nuv sync --dev\n\n\n\n\n\n\n“uv not found” - Ensure uv is installed and in your PATH - Restart your terminal after installation - Try the alternative pip installation method\n“Python version not supported” - Verify Python 3.13+ is installed: python --version - You may need to use python3.13 explicitly - Consider using pyenv to manage Python versions\n“OpenAI API key not configured” - Ensure .env file exists in the project root - Verify your API key is correct and has credits - Check for extra spaces or characters in the key\n“Permission denied” errors - On macOS/Linux, you may need to use chmod +x on scripts - Ensure you have write permissions in the installation directory\n“SSL Certificate” errors - Update certificates: pip install --upgrade certifi - Check corporate firewall/proxy settings\n\n\n\nIf you continue to experience issues:\n\nCheck system compatibility:\nuv run python -m veritascribe config\nEnable verbose logging:\nuv run python -m veritascribe test --verbose\nVerify environment:\nuv run python -c \"import sys; print(sys.version)\"\nuv run python -c \"from veritascribe import config; print('Import successful')\"\n\n\n\n\n\nNow that VeritaScribe is installed:\n\nConfigure your settings for optimal performance\nLearn the basic usage with examples\nReview the API reference for advanced usage\n\n\nInstallation complete! You’re ready to start analyzing thesis documents with VeritaScribe.",
    "crumbs": [
      "Home",
      "Installation Guide"
    ]
  },
  {
    "objectID": "installation.html#prerequisites",
    "href": "installation.html#prerequisites",
    "title": "Installation Guide",
    "section": "",
    "text": "Before installing VeritaScribe, ensure you have:\n\nPython 3.13 or higher: VeritaScribe requires modern Python features\nOpenAI API Key: Required for LLM-based analysis\nuv: Modern Python package manager (recommended)",
    "crumbs": [
      "Home",
      "Installation Guide"
    ]
  },
  {
    "objectID": "installation.html#step-1-check-python-version",
    "href": "installation.html#step-1-check-python-version",
    "title": "Installation Guide",
    "section": "",
    "text": "Verify you have Python 3.13 or higher:\npython --version\n# Should show Python 3.13.x or higher\nIf you need to install or upgrade Python:\n\nmacOSUbuntu/DebianWindows\n\n\n# Using Homebrew\nbrew install python@3.13\n\n# Using pyenv\npyenv install 3.13.0\npyenv global 3.13.0\n\n\n# Add deadsnakes PPA for latest Python versions\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt update\nsudo apt install python3.13 python3.13-venv python3.13-pip\n\n\nDownload Python 3.13 from python.org or use:\n# Using Chocolatey\nchoco install python --version=3.13.0\n\n# Using winget\nwinget install Python.Python.3.13",
    "crumbs": [
      "Home",
      "Installation Guide"
    ]
  },
  {
    "objectID": "installation.html#step-2-install-uv-recommended",
    "href": "installation.html#step-2-install-uv-recommended",
    "title": "Installation Guide",
    "section": "",
    "text": "uv is a fast Python package manager that VeritaScribe uses for dependency management:\n\nmacOS/LinuxWindowsAlternative (pip)\n\n\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n\npip install uv",
    "crumbs": [
      "Home",
      "Installation Guide"
    ]
  },
  {
    "objectID": "installation.html#step-3-clone-repository",
    "href": "installation.html#step-3-clone-repository",
    "title": "Installation Guide",
    "section": "",
    "text": "Clone the VeritaScribe repository:\ngit clone &lt;repository-url&gt;\ncd VeritaScribe",
    "crumbs": [
      "Home",
      "Installation Guide"
    ]
  },
  {
    "objectID": "installation.html#step-4-install-dependencies",
    "href": "installation.html#step-4-install-dependencies",
    "title": "Installation Guide",
    "section": "",
    "text": "Install all required dependencies using uv:\nuv sync\nThis will: - Create a virtual environment automatically - Install all dependencies specified in pyproject.toml - Set up the project for development",
    "crumbs": [
      "Home",
      "Installation Guide"
    ]
  },
  {
    "objectID": "installation.html#step-5-configure-environment",
    "href": "installation.html#step-5-configure-environment",
    "title": "Installation Guide",
    "section": "",
    "text": "Copy the example environment configuration:\ncp .env.example .env\n\n\n\nOpen .env in your preferred text editor and add your OpenAI API key:\n# Required: OpenAI API Configuration\nOPENAI_API_KEY=your_openai_api_key_here\n\n# Optional: Customize other settings as needed\nDEFAULT_MODEL=gpt-4\nMAX_TOKENS=2000\nTEMPERATURE=0.1\n\n\n\n\n\n\nOpenAI API Key Required\n\n\n\nVeritaScribe requires an OpenAI API key to function. You can obtain one from the OpenAI Platform.\nSecurity Note: Never commit your API key to version control. The .env file is already included in .gitignore.",
    "crumbs": [
      "Home",
      "Installation Guide"
    ]
  },
  {
    "objectID": "installation.html#step-6-verify-installation",
    "href": "installation.html#step-6-verify-installation",
    "title": "Installation Guide",
    "section": "",
    "text": "Test your installation with the built-in system test:\nuv run python -m veritascribe test\nThis will verify: - ✅ Configuration loading - ✅ PDF processing capabilities - ✅ LLM connectivity (if API key is configured)\nYou should see output similar to:\nRunning VeritaScribe system tests...\n✓ Configuration loading works\n✓ PDF processing works\n✓ Analysis modules work\n\nTest Results: 3/3 passed\n🎉 All tests passed! VeritaScribe is ready to use.",
    "crumbs": [
      "Home",
      "Installation Guide"
    ]
  },
  {
    "objectID": "installation.html#step-7-try-the-demo",
    "href": "installation.html#step-7-try-the-demo",
    "title": "Installation Guide",
    "section": "",
    "text": "Run the demo to see VeritaScribe in action:\nuv run python -m veritascribe demo\nThis will: 1. Create a sample thesis PDF 2. Perform a quick analysis 3. Display results and save example reports",
    "crumbs": [
      "Home",
      "Installation Guide"
    ]
  },
  {
    "objectID": "installation.html#alternative-installation-methods",
    "href": "installation.html#alternative-installation-methods",
    "title": "Installation Guide",
    "section": "",
    "text": "If you prefer to use pip directly:\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -e .\n\n\n\nFor development work, install with additional dev dependencies:\nuv sync --dev",
    "crumbs": [
      "Home",
      "Installation Guide"
    ]
  },
  {
    "objectID": "installation.html#troubleshooting-installation",
    "href": "installation.html#troubleshooting-installation",
    "title": "Installation Guide",
    "section": "",
    "text": "“uv not found” - Ensure uv is installed and in your PATH - Restart your terminal after installation - Try the alternative pip installation method\n“Python version not supported” - Verify Python 3.13+ is installed: python --version - You may need to use python3.13 explicitly - Consider using pyenv to manage Python versions\n“OpenAI API key not configured” - Ensure .env file exists in the project root - Verify your API key is correct and has credits - Check for extra spaces or characters in the key\n“Permission denied” errors - On macOS/Linux, you may need to use chmod +x on scripts - Ensure you have write permissions in the installation directory\n“SSL Certificate” errors - Update certificates: pip install --upgrade certifi - Check corporate firewall/proxy settings\n\n\n\nIf you continue to experience issues:\n\nCheck system compatibility:\nuv run python -m veritascribe config\nEnable verbose logging:\nuv run python -m veritascribe test --verbose\nVerify environment:\nuv run python -c \"import sys; print(sys.version)\"\nuv run python -c \"from veritascribe import config; print('Import successful')\"",
    "crumbs": [
      "Home",
      "Installation Guide"
    ]
  },
  {
    "objectID": "installation.html#next-steps",
    "href": "installation.html#next-steps",
    "title": "Installation Guide",
    "section": "",
    "text": "Now that VeritaScribe is installed:\n\nConfigure your settings for optimal performance\nLearn the basic usage with examples\nReview the API reference for advanced usage\n\n\nInstallation complete! You’re ready to start analyzing thesis documents with VeritaScribe.",
    "crumbs": [
      "Home",
      "Installation Guide"
    ]
  }
]